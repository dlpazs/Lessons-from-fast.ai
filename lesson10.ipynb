{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/cnn-fastai/blob/master/lesson10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaaSZ5MO5-cD",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "Where we are going :\n",
        "We have seen in every lesson this idea of taking a pre-trained model, whip off some some stuff on the top, replace it with something new, and get it to do something similar. We’ve kind of dived in a little bit deeper to that to say with ConvLearner.pretrained it had a standard way of sticking stuff on the top which does a particular thing (i.e. classification). Then we learned actually we can stick any PyTorch module we like on the end and have it do anything we like with a custom_head and so suddenly you discover there’s some really interesting things we can do.\n",
        "\n",
        "In fact, Yang Lu said “what if we did a different kind of custom head?” and the different custom head was let’s take the original pictures, rotate them, and the make our dependent variable the opposite of that rotation and see if it can learn to un-rotate it. This is a super useful thing, in fact, I think Google photos nowadays has this option that it’ll actually automatically rotate your photos for you. But the cool thing is, as he showed here, you can build that network right now by doing exactly the same as our previous lesson. But your custom head is one that spits out a single number which is how much to rotate by, and your dataset has a dependent variable which is how much you rotated by.\n",
        "\n",
        "So you suddenly realize with this idea of a backbone plus a custom head, you can do almost anything you can think about [16:30].\n",
        "\n",
        "Today, we are going to look at the same idea and see how that applies to NLP.\n",
        "In the next lesson, we are going to go further and say if NLP and computer vision lets you do the same basic ideas, how do we combine the two. We are going to learn about a model that can actually learn to find word structures from images, images from word structures, or images from images. That will form the basis if you wanted to go further of doing things like going from an image to a sentence (i.e. image captioning) or going from a sentence to an image which we kind of started to do, a phrase to image.\n",
        "From there, we’ve got to go deeper then into computer vision to think what other kinds of things we can do with this idea of pre-trained network plus a custom head. So we will look at various kinds of image enhancement like increasing the resolution of a low-res photo to guess what was missing or adding artistic filters on top of photos, or changing photos of horses into photos of zebras, etc.\n",
        "Then finally that’s going to bring us all the way back to bounding boxes again. To get there, we’re going to first of all learn about segmentation which is not just figuring out where a bounding box is, but figuring out what every single pixel in an image is a part of — so this pixel is a part of a person, this pixel is a part of a car. Then we are going to use that idea, particularly an idea called UNet, which turns out that this idea of UNet, we can apply to bounding boxes — where it’s called feature pyramids. We’ll use that to get really good results with bounding boxes. That’s kind of our path from here. It’s all going to build on each other but take us into lots of different areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVgAvpr6e8k",
        "colab_type": "text"
      },
      "source": [
        "## IMDB\n",
        "\n",
        "### Standardize format\n",
        "\n",
        "The basic paths for NLP is that we have to take sentences and turn them into numbers, and there is a couple to get there. At the moment, somewhat intentionally, fastai.text does not provide that many helper functions. It’s really designed more to let you handle things in a fairly flexible way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itt5WGtdlCfH",
        "colab_type": "code",
        "outputId": "b97edbc0-8b8f-4c1e-ad1a-e3874cdbdc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1511
        }
      },
      "source": [
        "!pip install fastai==0.7.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai==0.7.0 in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.1.7)\n",
            "Requirement already satisfied: widgetsnbextension in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.4.2)\n",
            "Requirement already satisfied: isoweek in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.3.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.2.2)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.10)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (16.0.4)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.6.0)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.1.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (7.4.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2018.10.15)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.4.3.18)\n",
            "Requirement already satisfied: bcolz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.3.0)\n",
            "Requirement already satisfied: feather-format in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.5.3)\n",
            "Requirement already satisfied: torch<0.4 in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.3.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.3.2)\n",
            "Requirement already satisfied: simplegeneric in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.8.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.5.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.19.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.1.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.0.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.0.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.14.6)\n",
            "Requirement already satisfied: sklearn-pandas in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.7.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.5.3)\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.4.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.4.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.6.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.3)\n",
            "Requirement already satisfied: pandas-summary in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.0.5)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.10.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.27.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2018.5)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.0.1)\n",
            "Requirement already satisfied: jedi in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.13.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension->fastai==0.7.0) (5.2.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->fastai==0.7.0) (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai==0.7.0) (1.11.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai==0.7.0) (0.46)\n",
            "Requirement already satisfied: pyarrow>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from feather-format->fastai==0.7.0) (0.11.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (5.4.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (4.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (6.0.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (39.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (1.0.15)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (4.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas->fastai==0.7.0) (0.19.2)\n",
            "Requirement already satisfied: geopandas>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.4.0)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (1.1.0)\n",
            "Requirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.8.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.5.0)\n",
            "Requirement already satisfied: mizani>=0.4.5 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.5.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->fastai==0.7.0) (5.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->fastai==0.7.0) (2.18.4)\n",
            "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from jedi->fastai==0.7.0) (0.3.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.0) (4.4.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.0) (0.8.1)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (0.5.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas>=0.3.0->plotnine->fastai==0.7.0) (1.6.4.post2)\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (from geopandas>=0.3.0->plotnine->fastai==0.7.0) (1.7.13)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.6/dist-packages (from geopandas>=0.3.0->plotnine->fastai==0.7.0) (1.9.5.1)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.6/dist-packages (from mizani>=0.4.5->plotnine->fastai==0.7.0) (3.1.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (1.22)\n",
            "Requirement already satisfied: cligj>=0.4 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.0) (0.5.0)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.0) (1.0.4)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.0) (2.3.2)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from cligj>=0.4->fiona->geopandas>=0.3.0->plotnine->fastai==0.7.0) (7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GuJGBIMxyAT",
        "colab_type": "code",
        "outputId": "b0e12e12-fb28-41a6-cec2-d911639d02b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "!pip install torchtext==0.2.3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.2.3 in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.27.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (1.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ffKRgl_kad9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *\n",
        "import html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ArgwzbdlBSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH=Path('data/')\n",
        "DATA_PATH.mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lbi9Cq4mEOD",
        "colab_type": "code",
        "outputId": "48c6e18c-9cf6-4129-86b3-63d4f891e19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-16 17:24:18--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.2’\n",
            "\n",
            "aclImdb_v1.tar.gz.2 100%[===================>]  80.23M  27.4MB/s    in 2.9s    \n",
            "\n",
            "2018-10-16 17:24:21 (27.4 MB/s) - ‘aclImdb_v1.tar.gz.2’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szkGvwmfmLP8",
        "colab_type": "code",
        "outputId": "1bb0a3ee-c125-4e68-881a-8dd57477da1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1733680
        }
      },
      "source": [
        "!tar xzfv aclImdb_v1.tar.gz "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWD9GQ2pnWxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv aclImdb data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In2fPt7qmTJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS = 'xbos' # beginning-of-sentence tag\n",
        "FLD = 'xfld' # data field tag\n",
        "\n",
        "PATH=Path('data/aclImdb/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmvenWT-niIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLAS_PATH=Path('data/imdb_clas/')\n",
        "CLAS_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "LM_PATH=Path('data/imdb_lm/')\n",
        "LM_PATH.mkdir(exist_ok=True)\n",
        "#the classification path is going to contain the info used to create a sentiment analysis path\n",
        "#the Lang model path is containing info to create LM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnjmvpmB6iRO",
        "colab_type": "text"
      },
      "source": [
        "As you can see here [21:59], I wrote something called get_texts which goes through each thing in CLASSES. There are three classes in IMDb: negative, positive, and then there’s another folder “unsupervised” which contains the ones they haven’t gotten around to labeling yet — so we will just call that a class for now. So we just go through each one of those classes, then find every file in that folder, and open it up, read it, and chuck it into the end of the array. As you can see, with pathlib, it’s super easy to grab stuff and pull it in, and then the label is just whatever class we are up to so far. We will do that for both training set and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRcbek-Pn-kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES = ['neg', 'pos', 'unsup']\n",
        "\n",
        "def get_texts(path):\n",
        "    texts,labels = [],[]\n",
        "    for idx,label in enumerate(CLASSES): # go through each class\n",
        "        for fname in (path/label).glob('*.*'): # find every file in that folder with that name\n",
        "            texts.append(fname.open('r', encoding='utf-8').read()) # open it and read it and chuck it into the end of this array\n",
        "            labels.append(idx) # the label is just whatever class i'm up to so far\n",
        "    return np.array(texts),np.array(labels)\n",
        "\n",
        "trn_texts,trn_labels = get_texts(PATH/'train')\n",
        "val_texts,val_labels = get_texts(PATH/'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBxglWjfomHV",
        "colab_type": "code",
        "outputId": "3624089d-b4db-4265-d1ed-6ccbef04cfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(trn_texts), len(val_texts) # 50,00 of train are unsup "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SZUJkXyoxZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_names = ['labels', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z44SUK56uYU",
        "colab_type": "text"
      },
      "source": [
        "One thing that’s always good idea is to sort things randomly [23:19]. It is useful to know this simple trick for sorting things randomly particularly when you’ve got multiple things you have to sort the same way. In this case, you have labels and texts. np.random.permutation, if you give it an integer, it gives you a random list from 0 up to and not including the number you give it in some random order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzlBc9zNo1Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42) # next step is to sort randomly as its good idea\n",
        "trn_idx = np.random.permutation(len(trn_texts)) # np rand perm if you give it an int it \n",
        "#gives you a random list from 0 up to \n",
        "# and not including the num you give it in some random order\n",
        "val_idx = np.random.permutation(len(val_texts))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qELgxWuE6z-l",
        "colab_type": "text"
      },
      "source": [
        "You can them pass that in as an indexer to give you a list that’s sorted in that random order. So in this case, it is going to sort trn_texts and trn_labels in the same random way. So that’s a useful little idiom to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bry9DUl_o-vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_texts = trn_texts[trn_idx] # you can pass that in as an indexer\n",
        "val_texts = val_texts[val_idx] #to give you a list thats sorted in that random order\n",
        "\n",
        "trn_labels = trn_labels[trn_idx] # its going to sort them both in the same random way\n",
        "val_labels = val_labels[val_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bS4U-2R62C8",
        "colab_type": "text"
      },
      "source": [
        "Now we have our texts and labels sorted, we can create a dataframe from them [24:07]. Why are we doing this? The reason is because there is a somewhat standard approach starting to appear for text classification datasets which is to have your training set as a CSV file with the labels first, and the text of the NLP documents second. So it basically looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuBHQiqFpKBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trn = pd.DataFrame({'text': trn_texts, 'labels':trn_labels}, columns=col_names) # now that we have the texts and labels sorted\n",
        "#we can then create a dataframe from them.\n",
        "#why though? because, there is a standard approach starting to appear for text classification datasets \n",
        "#that is to have your train dataset in a csv file. With the labels first and the text second in their respective csv's\n",
        "#and a file with classes.txt that lists the classes\n",
        "df_val = pd.DataFrame({'text': val_texts, 'labels':val_labels}, columns=col_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RYlhA3D693G",
        "colab_type": "text"
      },
      "source": [
        "So you have your labels and texts, and then a file called classes.txt which just lists the classes. I say somewhat standard because in a reasonably recent academic paper Yann LeCun and a team of researcher looked at quite a few datasets and they use this format for all of them. So that’s what I started using as well for my recent paper. You’ll find that this notebook, if you put your data into this format, the whole notebook will work every time [25:17]. So rather than having a thousand different formats, I just said let’s just pick a standard format and your job is to put your data in that format which is the CSV file. The CSV files have no header by default.\n",
        "\n",
        "You’ll notice at the start, we have two different paths [25:51]. One was the classification path, and the other was the language model path. In NLP, you’ll see LM all the time. LM means language model. The classification path is going to contain the information that we are going to use to create a sentiment analysis model. The language model path is going to contain the information we need to create a language model. So they are a little bit different. One thing that is different is that when we create the train.csv in the classification path, we remove everything that has a label of 2 because label of 2 is “unsupervised” and we can’t use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnkSn3CtpaCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train.csv', header=False, index=False)#when we create the train.csv in the classification path\n",
        "#we remove everything that has a label of 2, because label of 2 is unsupervised\n",
        "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)#the labels for the classification path are the actual labels\n",
        "#the labels for the LM path have no labels so \n",
        "\n",
        "(CLAS_PATH/'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjtRQ6EFuHEN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We start by creating the data for the Language Model(LM). The **LM's goal is to learn the structure of the english language.** It **learns language by trying to predict the next word given a set of previous words(ngrams)**. Since the LM does not classify reviews, the labels can be ignored.\n",
        "\n",
        "The LM can benefit from all the textual data and there is no need to exclude the unsup/unclassified movie reviews.\n",
        "\n",
        "We first concat all the train(pos/neg/unsup = 75k) and test(pos/neg=25k) reviews into a big chunk of 100k reviews. And then we use sklearn splitter to divide up the 100k texts into 90% training and 10% validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrdUISAVp1yj",
        "colab_type": "code",
        "outputId": "0fb8425a-6902-4341-a030-9b37ac2e7b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df_trn.head()\n",
        "#youll find that with this notebook is that if you've put your data into this format the whole notebook will work everytime\n",
        "#Your job is to put it into a csv file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>This is one of the funniest films I've ever se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I don't see much reason to get into this movie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Wes Craven has been created a most successful ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>The images are amazing! Clearly, the filmed cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>I wasn't interested in the story, mainly becau...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   labels                                               text\n",
              "0       2  This is one of the funniest films I've ever se...\n",
              "1       0  I don't see much reason to get into this movie...\n",
              "2       1  Wes Craven has been created a most successful ...\n",
              "3       2  The images are amazing! Clearly, the filmed cl...\n",
              "4       2  I wasn't interested in the story, mainly becau..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETm37Pq0ykEd",
        "colab_type": "code",
        "outputId": "65554fe5-70fb-46f1-8c4e-06e6017869e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(CLAS_PATH/'classes.txt').open().readlines()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg\\n', 'pos\\n', 'unsup\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QmtClXN7LXm",
        "colab_type": "text"
      },
      "source": [
        "Now the language model, we can create our own validation set, so you’ve probably come across by now, sklearn.model_selection.train_test_split which is a really simple function that grabs a dataset and randomly splits it into a training set and a validation set according to whatever proportion you specify. In this case, we concatenate our classification training and validation together, split it by 10%, now we have 90,000 training, 10,000 validation for our language model. So that’s getting the data in a standard format for our language model and our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzPIgX3C6fAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fastinotwork = np.concatenate([trn_texts,val_texts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjcHPICo50TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trn_texts,val_texts = train_test_split(fastinotwork, test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxyBkmAH0IPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_texts,val_texts = sklearn.model_selection.train_test_split(\n",
        "    np.concatenate([trn_texts,val_texts]), test_size=0.1)#simple function that splits data\n",
        "#and we concat classification training and testing and split it by 10%.\n",
        "#100,000 altogether so 90,000 train and 10,000 for validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLA125DS0VMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(trn_texts), len(val_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dwMfvBj7JS3",
        "colab_type": "text"
      },
      "source": [
        "The second difference is the labels [26:51]. For the classification path, the labels are the actual labels, but for the language model, there are no labels so we just use a bunch of zeros and that just makes it a little easier because we can use a consistent dataframe/CSV format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_orAsd0eWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)#no labels \n",
        "#for the lm and we just use zeros - easier for csv format\n",
        "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
        "\n",
        "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
        "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usxu3bmn34jh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Language model tokens\n",
        "In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
        "\n",
        "1. Clean up extra spaces, tab chars, new ln chars and other characters and replace them with standard ones\n",
        "2. Use the awesome spacy library to tokenize the data. Since spacy does not provide a parallel/multicore version of the tokenizer, the fastai library adds this functionality. This parallel version uses all the cores of your CPUs and runs much faster than the serial version of the spacy tokenizer.\n",
        "Tokenization is the process of splitting the text into separate tokens so that each token can be assigned a unique index. This means we can convert the text into integer indexes our models can use.\n",
        "\n",
        "We use an appropriate chunksize as the tokenization process is memory intensive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ6eRTah5AbG",
        "colab_type": "text"
      },
      "source": [
        "The next thing we need to do is tokenization. Tokenization means at this stage, for a document (i.e. a movie review), we have a big long string and we want to turn it into a list of tokens which is similar to a list of words but not quite. For example, don’t we want it to be do and n’t, we probably want full stop to be a token, and so forth. Tokenization is something that we passed off to a terrific library called spaCy — partly terrific because the Australian wrote it and partly terrific because it’s good at what it does. We put a bit of stuff on top of spaCy but the vast majority of the work’s been done by spaCy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAOEcq1Q3iSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#next thing to do is tokenization\n",
        "#that means that at this stage we have, for a document, a big long string\n",
        "#and we want to turn it into a list of tokens which are kind of a list of words but not quite\n",
        "chunksize=24000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HMDYmdy5Lt_",
        "colab_type": "text"
      },
      "source": [
        "Before we pass it to spaCy, Jeremy wrote this simple fixup function which is each time he’s looked at different datasets (about a dozen in building this), every one had different weird things that needed to be replaced. So here are all the ones he’s come up with so far, and hopefully this will help you out as well. All the entities are html unescaped and there are bunch more things we replace. Have a look at the result of running this on text that you put in and make sure there’s no more weird tokens in there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8BC03LF5MCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re1 = re.compile(r'  +')\n",
        "\n",
        "def fixup(x):\n",
        "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x)) #html escape turn & into &amp; and unescape turn & into &"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYl7H9S27_6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_texts(df, n_lbls=1):\n",
        "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)#get texts is going to grab the labels make them \n",
        "    #into ints. \n",
        "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)#its gna grab the texts. Beginning of stream token=BOS\n",
        "    #type of texts that\n",
        "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)#allows us\n",
        "      #to have multiple fields in our csv\n",
        "    texts = list(texts.apply(fixup).values)#applies fixup\n",
        "\n",
        "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))#tokenize it by process all multi-processing\n",
        "    return tok, list(labels)#because tokenizing is slow and spacy is slow\n",
        "  #each part of the list will be tok on each core. Partition by cores which takes a list and splits it\n",
        "  #into sublists which the num of cores on your comp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIHCbv028HxX",
        "colab_type": "text"
      },
      "source": [
        "get_all function calls get_texts and get_texts is going to do a few things [29:40]. One of which is to apply that fixup that we just mentioned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_URaep8Obu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all(df, n_lbls):\n",
        "    tok, labels = [], []\n",
        "    for i, r in enumerate(df):#we go through each chunk of which each one is a dataframe\n",
        "        print(i)\n",
        "        tok_, labels_ = get_texts(r, n_lbls)#and we call get texts\n",
        "        tok += tok_;\n",
        "        labels += labels_\n",
        "    return tok, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBE_KERc8rWw",
        "colab_type": "text"
      },
      "source": [
        "Let’s look through this because there is some interesting things to point out [29:57]. We are going to use pandas to open our train.csv from the language model path, but we are passing in an extra parameter you may not have seen before called **chunksize**. **Python and pandas can both be pretty inefficient** when it comes to storing and using text data. So you’ll see that very few people in NLP are working with large corpuses. And Jeremy thinks the part of the reason is that traditional tools made it really difficult — you run out of memory all the time. So this process he is showing us today, he has used on corpuses of over a billion words successfully using this exact code. One of the simple trick is this thing called chunksize with pandas. That that means **is that pandas does not return a data frame, but it returns an iterator that we can iterate through chunks of a data frame.** That is why we don’t say tok_trn = get_text(df_trn) but instead **we call get_all which loops through the data frame but actually what it’s really doing is it’s looping through chunks of the data frame so each of those chunks is basically a data frame representing a subset of the data **[31:05]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UGfG3UC8s7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
        "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4p45s5v9MiC",
        "colab_type": "text"
      },
      "source": [
        "Question: When I’m working with NLP data, many times I come across data with foreign texts/characters. Is it better to discard them or keep them [31:31]? No no, definitely keep them. This whole process is unicode and I’ve actually used this on Chinese text. This is designed to work on pretty much anything. In general, most of the time, it’s not a good idea to remove anything. Old-fashioned NLP approaches tended to do all this like lemmatization and all these normalization steps to get rid things, lower case everything, etc. But that’s throwing away information which you don’t know ahead of time whether it’s useful or not. So don’t throw away information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6MI15x9PJ_",
        "colab_type": "text"
      },
      "source": [
        "So we go through each chunk each of which is a data frame and we call get_texts [32:19]. get_texts will grab the labels and makes them into integers, and it’s going to grab the texts. A couple things to point out:\n",
        "\n",
        "- Before we include the text, we have “beginning of stream” (BOS) token which we defined in the beginning. There’s nothing special about these particular strings of letters — **they are just ones I figured don’t appear in normal texts very often**. So every text is going to start with ‘**xbos’ — why is that? Because it’s often useful for your model to know when a new text is starting.** For example, if it’s a language model, **we are going to concatenate all the texts together. So it would be really helpful for it to know all this articles finished and a new one started so I should probably forget some of their context now.**\n",
        "\n",
        "- Ditto is quite often texts have multiple fields like a title and abstract, and then a main document. So by the same token, we’ve got this thing here which lets us actually have multiple fields in our CSV. So this process is designed to be very flexible. Again at the start of each one, we put a special “field starts here” token followed by the number of the field that’s starting here for as many fields as we have. Then we apply fixup to it.\n",
        "\n",
        "- Then most importantly [33:54], we tokenize it — we tokenize it by doing a “process all multiprocessing” (proc_all_mp). Tokenizing tends to be pretty slow but we’ve all got multiple cores in our machines now, and some of the better machines on AWS can have dozens of cores. spaCy is not very amenable to multi processing but Jeremy finally figured out how to get it to work. The good news is that it’s all wrapped up in this one function now. So all you need to pass to that function is a list of things to tokenize which each part of that list will be tokenized on a different core. There is also a function called partition_by_cores which takes a list and splits it into sublists. The number of sublists is the number of cores that you have in your computer. On Jeremy’s machine without multiprocessing, this takes about an hour and a half, and with multiprocessing, it takes about 2 minutes. So it’s a really hand thing to have. Feel free to look inside it and take advantage of it for your own stuff. Remember, we all have multiple cores even in our laptops and very few things in Python take advantage or it unless you make a bit of an effort to make it work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luYzXkTs9NBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_trn, trn_labels = get_all(df_trn, 1)\n",
        "tok_val, val_labels = get_all(df_val, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oxjgsl3-m9k",
        "colab_type": "text"
      },
      "source": [
        "Here is the result at the end [35:42]. Beginning of the stream token (xbos), beginning of field number 1 token (xfld 1), and tokenized text. You’ll see that the punctuation is on whole now a separate token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIWNJDOs-opr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(LM_PATH/'tmp').mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngCCZxDA-5Hn",
        "colab_type": "text"
      },
      "source": [
        "t_up: t_up mgm — MGM was originally capitalized. But the interesting thing is that normally people either lowercase everything or they leave the case as is. **Now if you leave the case as is, then “SCREW YOU” and “screw you” are two totally different sets of tokens that have to be learnt from scratch. Or if you lowercase them all, then there is no difference at all. So how do you fix this so that you both get a semantic impact of “I’M SHOUTING NOW” but not have to learn the shouted version vs. the normal version.** So the idea is to come up with **a unique token to mean the next thing is all uppercase. Then we lowercase it, so now whatever used to be uppercase is lowercased, and then we can learn the semantic meaning of all uppercase.**\n",
        "\n",
        "tk_rep: Similarly, if you have 29 ! in a row, we don’t learn a separate token for 29 exclamation marks — instead we put in a** special token for “the next thing repeats lots of times” and then put the number 29 and an exclamation mark (i.e. tk_rep 29 !).** So there are a few tricks like that. If you are interested in NLP, have a look at the tokenizer code for these little tricks that Jeremy added in because some of them are kind of fun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X27AuFhE-0YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "' '.join(tok_trn[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpEF6JxI_rKH",
        "colab_type": "text"
      },
      "source": [
        "The **nice thing with doing things this way is we can now just np.save that and load it back up later** [37:44]. We don’t have to recalculate all this stuff each time like we tend to have to do with torchtext or a lot of other libraries. **Now that we got it tokenized, the next thing we need to do is to turn it into numbers which we call numericalizing it. The way we numericalize it is very simple.**\n",
        "\n",
        "- We make a list list of all the words that appear in some order\n",
        "- Then we replace every word with its index into that list\n",
        "- The list of all the tokens, we call that the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-uxvE2M-qLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
        "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI9iEFlL-sl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
        "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b_nlSSI_4HI",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of some of the vocabulary [38:28]. The **Counter class in Python is very handy for this. It basically gives us a list of unique items and their counts.** Here are the 25 most common things in the vocabulary.** Generally speaking, we don’t want every unique token in our vocabulary. If it doesn’t appear at least twice then might just be a spelling mistake or a word we can’t learn anything about it if it doesn’t appear that often.** Also the stuff we are going to be learning about so far in this part **gets a bit clunky once you’ve got a vocabulary bigger than 60,000**. Time permitting, we may look at some work Jeremy has been doing recently on handling larger vocabularies, otherwise that might have to come in a future course. But actually for classification, doing more than about 60,000 words doesn’t seem to help anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cgk1vDWAS3s",
        "colab_type": "text"
      },
      "source": [
        "The vocab is the unique set of all tokens in our dataset. The vocab provides us a way for us to simply replace each word in our datasets with a unique integer called an index.\n",
        "\n",
        "In a large corpus of data one might find some rare words which are only used a few times in the whole dataset. We discard such rare words and avoid trying to learn meaningful patterns out of them.\n",
        "\n",
        "Here we have set a minimum frequency of occurence to 2 times. It has been observed by NLP practicioners that a maximum vocab of 60k usually yields good results for classification tasks. So we set maz_vocab to 60000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXYYN6ta-ucP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq = Counter(p for o in tok_trn for p in o)\n",
        "freq.most_common(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEaT-uCAVxV",
        "colab_type": "text"
      },
      "source": [
        "So we are going to limit our vocabulary to 60,000 words, things that appear at least twice [39:33]. Here is a simple way to do that. **Use .most_common, pass in the max vocab size.** **That’ll sort it by the frequency and if it appears less often than a minimum frequency, then don’t bother with it at all.** That gives us itos — that’s the same name that torchtext used and it means integer-to-string. This is just the list of unique tokens in the vocab. W**e’ll insert two more tokens — a vocab item for unknown (_unk_) and a vocab item for padding (_pad_).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WpnB1XQAXPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_vocab = 60000\n",
        "min_freq = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDFYQB7_AZVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]# int to string itos\n",
        "itos.insert(0, '_pad_')\n",
        "itos.insert(0, '_unk_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSTgcu7YA1wB",
        "colab_type": "text"
      },
      "source": [
        "We create a reverse mapping called stoi which is useful to lookup the index of a given token. stoi also has the same number of elements as itos. We use a high performance container called collections.defaultdict to store our stoi mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT2cgTF6A5zO",
        "colab_type": "text"
      },
      "source": [
        "We can then create the dictionary which goes in the opposite direction (string to integer)[40:19]. That won’t cover everything because we intentionally truncated it down to 60,000 words. **If we come across something that is not in the dictionary, we want to replace it with zero for unknown so we can use defaultdict with a lambda function that always returns zero.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ZtWi4yA3lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
        "len(itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlwi2Ez5BGHe",
        "colab_type": "text"
      },
      "source": [
        "So now we have our stoi dictionary defined, we can then call that for every word for every sentence [40:50]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNq6YqwZBEhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
        "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvLikUoaBXWD",
        "colab_type": "text"
      },
      "source": [
        "Here is our numericalized version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ2UfGaYBRGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "' '.join(str(o) for o in trn_lm[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ih4qQTPBamw",
        "colab_type": "text"
      },
      "source": [
        "Of course, the nice thing is we can save that step as well. Each time we get to another step, we can save it. These are not very big files compared to what you are used with images. Text is generally pretty small.\n",
        "\n",
        "Very important to also save that vocabulary (itos). **The list of numbers means nothing unless you know what each number refers to, and that’s what itos tells you.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvCiTbh7BG0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
        "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
        "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sv3BvauCAuc",
        "colab_type": "text"
      },
      "source": [
        "So you save those three things, and later on you can load them back up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_RnQbfaBK1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
        "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
        "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztw0_GJrCCbl",
        "colab_type": "text"
      },
      "source": [
        "Now our vocab size is 60,002 and our training language model has 90,000 documents in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve2us7LYBMQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vs=len(itos)\n",
        "vs,len(trn_lm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OC5eR8ZCElG",
        "colab_type": "text"
      },
      "source": [
        "That’s the preprocessing you do [42:01]. We can probably wrap a little bit more of that in utility functions if we want to but it’s all pretty straight forward and that exact code will work for any dataset you have once you’ve got it in that CSV format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1xgbNXHCI7m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## wikitext103 conversion\n",
        "We are now going to build an english language model for the IMDB corpus. We could start from scratch and try to learn the structure of the english language. But we use a technique called transfer learning to make this process easier. In transfer learning (a fairly recent idea for NLP) a pre-trained LM that has been trained on a large generic corpus(like wikipedia articles) can be used to transfer it's knowledge to a target LM and the weights can be fine-tuned.\n",
        "\n",
        "Our source LM is the wikitext103 LM created by Stephen Merity @ Salesforce research. Link to dataset The language model for wikitext103 (AWD LSTM) has been pre-trained and the weights can be downloaded here: Link. Our target LM is the IMDB LM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE5mJxn8CgYn",
        "colab_type": "text"
      },
      "source": [
        "Here is kind of a new insight that’s not new at all which is that **we’d like to pre-train something.** **We know from lesson 4 that if we pre-train our classifier by first creating a language model and then fine-tuning that as a classifier, that was helpful. It actually got us a new state-of-the-art result — we got the best IMDb classifier result that had been published by quite a bit.** We are not going that far enough though, because IMDb movie reviews are not that different to any other English document; compared to how different they are to a random string or even to a Chinese document. So just like ImageNet allowed us to train things that recognize stuff that kind of looks like pictures, and we could use it on stuff that was nothing to do with ImageNet like satellite images. **Why don’t we train a language model that’s good at English and then fine-tune it to be good at movie reviews.**\n",
        "\n",
        "So this basic insight led Jeremy to try building a language model on Wikipedia. Stephen Merity has already processed Wikipedia, found a subset of nearly the most of it, but throwing away the stupid little articles leaving bigger articles. He calls that wikitext103. Jeremy grabbed wikitext103 and trained a language model on it. He used exactly the same approach he’s about to show you for training an IMDb language model, but instead he trained a wikitext103 language model. He saved it and made it available for anybody who wants to use it at this URL. **The idea now is let’s train an IMDb language model which starts with these weights.** Hopefully to you folks, this is an extremely obvious, extremely non-controversial idea because it’s basically what we’ve done in nearly every class so far. But when Jeremy first mentioned this to people in the NLP community June or July of last year, there couldn’t have been less interest and was told it was stupid [45:03]. Because Jeremy was obstreperous, he ignored them even though they know much more about NLP and tried it anyway. And let’s see what happened.\n",
        "\n",
        "**wikitext103 conversion** [46:11]\n",
        "Here is how we do it. Grab the wikitext models. If you do wget -r, it will recursively grab the whole directory which has a few things in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5_d4I6GCU6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/\n",
        "#wikitext 103 language model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRADfVUKCbs-",
        "colab_type": "text"
      },
      "source": [
        "The pre-trained LM weights have an embedding size of 400, 1150 hidden units and just 3 layers. We need to match these values with the target IMDB LM so that the weights can be loaded up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiByn_OgDUn5",
        "colab_type": "text"
      },
      "source": [
        "We need to make sure that our language model has exactly the same embedding size, number of hidden, and number of layers as Jeremy’s wikitext one did otherwise you can’t load the weights in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6KSqloWCdCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "em_sz,nh,nl = 400,1150,3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJioXAScDWfV",
        "colab_type": "text"
      },
      "source": [
        "Here are our pre-trained path and our pre-trained language model path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgqIJczeDr4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRE_PATH = PATH/'models'/'wt103'\n",
        "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0xtAJuDrZG",
        "colab_type": "text"
      },
      "source": [
        "Let’s go ahead and torch.load in those weights from the forward wikitext103 model. **We don’t normally use torch.load, but that’s the PyTorch way of grabbing a file**. It basically gives you a dictionary containing the name of the layer and a tensor/array of those weights.\n",
        "\n",
        "**Now the problem is that wikitext language model was built with a certain vocabulary which was not the same as ours** [47:14]. Our #40 is not the same as wikitext103 model’s #40. **So we need to map one to the other.** That’s very very simple because luckily Jeremy saved itos for the wikitext vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u167TZ6mDv09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG9d7bqzDyRR",
        "colab_type": "text"
      },
      "source": [
        "We calculate the mean of the layer0 encoder weights. This can be used to assign weights to unknown tokens when we transfer to target IMDB LM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5grFF7OKD16R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
        "row_m = enc_wgts.mean(0)#average embedding weight across wikitext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMJcT1ZuD3f4",
        "colab_type": "text"
      },
      "source": [
        "**Here is the list of what each word is for wikitext103 model, and we can do the same defaultdict trick to map it in reverse.** We’ll use -1 to mean that it is not in the wikitext dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQGvxhwD5Gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
        "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8aeJ1aJD6YR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Before we try to transfer the knowledge from wikitext to the IMDB LM, we match up the vocab words and their indexes. We use the defaultdict container once again, to assign mean weights to unknown IMDB tokens that do not exist in wikitext103."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2irYsflrD9Ka",
        "colab_type": "text"
      },
      "source": [
        "**So now we can just say our new set of weights is just a whole bunch of zeros with vocab size by embedding size (i.e. we are going to create an embedding matrix)** [47:57]. **We then go through every one of the words in our IMDb vocabulary.** **We are going to look it up in stoi2 (string-to-integer for the wikitext103 vocabulary) and see if it’s a word there.** **If that is a word there, then we won’t get the -1. So r will be greater than or equal to zero, so in that case, we will just set that row of the embedding matrix to the weight which was stored inside the named element ‘0.encoder.weight’.** You can look at this dictionary wgts and it’s pretty obvious what each name corresponds to. It looks very similar to the names that you gave it when you set up your module, so here are the encoder weights.\n",
        "\n",
        "**If we don’t find it [49:02], we will use the row mean — in other words, here is the average embedding weight across all of the wikitext103. So we will end up with an embedding matrix for every word that’s in both our vocabulary for IMDb and the wikitext103 vocab, we will use the wikitext103 embedding matrix weights; for anything else, we will just use whatever was the average weight from the wikitext103 embedding matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN9Fgd5kD_B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
        "for i,w in enumerate(itos):\n",
        "    r = stoi2[w]\n",
        "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2pl0lLDE-5j",
        "colab_type": "text"
      },
      "source": [
        "We now overwrite the weights into the wgts odict. The decoder module, which we will explore in detail is also loaded with the same weights due to an idea called weight tying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF8I_L31E7Lz",
        "colab_type": "text"
      },
      "source": [
        "We will then replace the encoder weights with new_w turn into a tensor [49:35]. We haven’t talked much about **weight tying, but basically the decoder (the thing that turns the final prediction back into a word) uses exactly the same weights, so we pop it there as well.** Then there is a bit of weird thing with how we do embedding dropout that ends up with a whole separate copy of them for a reason that doesn’t matter much. So we popped the weights back where they need to go. So this is now a set of torch state which we can load in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpO65jAE9F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wgts['0.encoder.weight'] = T(new_w)\n",
        "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
        "wgts['1.decoder.weight'] = T(np.copy(new_w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LySgERtgFVvI",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the weights prepared, we are ready to create and start training our new IMDB language pytorch model!\n",
        "\n",
        "## Language model\n",
        "It is fairly straightforward to create a new language model using the fastai library. Like every other lesson, our model will have a backbone and a custom head. The backbone in our case is the IMDB LM pre-trained with wikitext and the custom head is a linear classifier. In this section we will focus on the backbone LM and the next section will talk about the classifier custom head.\n",
        "\n",
        "bptt (also known traditionally in NLP LM as ngrams) in fastai LMs is approximated to a std. deviation around 70, by perturbing the sequence length on a per-batch basis. This is akin to shuffling our data in computer vision, only that in NLP we cannot shuffle inputs and we have to maintain statefulness.\n",
        "\n",
        "Since we are predicting words using ngrams, we want our next batch to line up with the end-points of the previous mini-batch's items. batch-size is constant and but the fastai library expands and contracts bptt each mini-batch using a clever stochastic implementation of a batch. (original credits attributed to Smerity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKRt06F-FbNH",
        "colab_type": "text"
      },
      "source": [
        "## Language model [50:18]\n",
        "Let’s create our language model. Basic approach we are going to use is we are going to concatenate all of the documents together into a single list of tokens of length 24,998,320. That is going to be what we pass in as a training set. So for the language model:\n",
        "\n",
        "- We take all our documents and just concatenate them back to back.\n",
        "- We are going to be continuously trying to predict what’s the next word after these words.\n",
        "- We will set up a whole bunch of dropouts.\n",
        "- Once we have a model data object, we can grab the model from it, so that’s going to give us a learner.\n",
        "- Then as per usual, we can call learner.fit. We do a single epoch on the last layer just to get that okay. The way it’s set up is the last layer is the embedding words because that’s obviously the thing that’s going to be the most wrong because a lot of those embedding weights didn’t even exist in the vocab. So we will train a single epoch of just the embedding weights.\n",
        "- Then we’ll start doing a few epochs of the full model. How is it looking? In lesson 4, we had the loss of 4.23 after 14 epochs. In this case, we have 4.12 loss after 1 epoch. So by pre-training on wikitext103, we have a better loss after 1 epoch than the best loss we got for the language model otherwise.\n",
        "\n",
        "Question: What is the wikitext103 model? Is it a AWD LSTM again [52:41]? Yes, we are about to dig into that. The way I trained it was literally the same lines of code that you see above, but without pre-training it on wikitext103."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nK6EIQqFh4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wd=1e-7\n",
        "bptt=70\n",
        "bs=52\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0slCb5RGEqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = len(np.concatenate(trn_lm))\n",
        "t, t//64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAWK-UkXGNe6",
        "colab_type": "text"
      },
      "source": [
        "The goal of the LM is to learn to predict a word/token given a preceeding set of words(tokens). We take all the movie reviews in both the 90k training set and 10k validation set and concatenate them to form long strings of tokens. In fastai, we use the LanguageModelLoader to create a data loader which makes it easy to create and use bptt sized mini batches. The LanguageModelLoader takes a concatenated string of tokens and returns a loader.\n",
        "\n",
        "We have a special modeldata object class for LMs called LanguageModelData to which we can pass the training and validation loaders and get in return the model itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtQ6W7OIHX0t",
        "colab_type": "text"
      },
      "source": [
        "This is the LanguageModelLoader and I really hope that by now, you’ve learned in your editor or IDE how to jump to symbols [1:02:37]. I don’t want it to be a burden for you to find out what the source code of LanguageModelLoader is. If your editor doesn’t make it easy, don’t use that editor anymore. There’s lots of good free editors that make this easy.\n",
        "\n",
        "So this is the source code for LanguageModelLoader, and it’s interesting to notice that it’s not doing anything particularly tricky. It’s not deriving from anything at all. What makes something that’s capable of being a data loader is that it’s something you can iterate over.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1500/1*ttM96lLbHQn06byFwmHj0g.png)\n",
        "\n",
        "Here is the fit function inside fastai.model [1:03:41]. This is where everything ends up eventually which goes through each epoch, creates an iterator from the data loader, and then just does a for loop through it. So anything you can do a for loop through can be a data loader. Specifically it needs to return tuples of independent and dependent variables for mini-batches.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*560U29nWI0xNGLsHgnWFNQ.png)\n",
        "\n",
        "So anything with a __iter__ method is something that can act as an iterator [1:04:09]. yield is a neat little Python keywords you probably should learn about if you don’t already know it. But it basically spits out a thing and waits for you to ask for another thing — normally in a for loop or something. In this case, we start by initializing the language model passing it in the numbers nums this is the numericalized long list of all of our documents concatenated together. The first thing we do is to “batchfy” it. This is the thing which quite a few of you got confused about last time. If our batch size is 64 and we have 25 million numbers in our list. We are not creating items of length 64 — we are creating 64 items in total. So each of them is of size t divided by 64 which is 390k. So that’s what we do here:\n",
        "\n",
        "\n",
        "Find rest at this link - way too much text for this Notebook\n",
        "\n",
        "https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01moKEmhHhUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
        "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
        "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qfMhkLCHuwF",
        "colab_type": "text"
      },
      "source": [
        "We setup the dropouts for the model - these values have been chosen after experimentation. If you need to update them for custom LMs, you can change the weighting factor (0.7 here) based on the amount of data you have. For more data, you can reduce dropout factor and for small datasets, you can reduce overfitting by choosing a higher dropout factor. No other dropout value requires tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SKufyQH8Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjULWHuFH9rs",
        "colab_type": "text"
      },
      "source": [
        "We first tune the last embedding layer so that the missing tokens initialized with mean weights get tuned properly. So we freeze everything except the last layer.\n",
        "\n",
        "We also keep track of the accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKv9y1EXH--2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
        "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
        "\n",
        "learner.metrics = [accuracy]\n",
        "learner.freeze_to(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Q0sAppIAkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.model.load_state_dict(wgts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPQd817ICYq",
        "colab_type": "text"
      },
      "source": [
        "We set learning rates and fit our IMDB LM. We first run one epoch to tune the last layer which contains the embedding weights. This should help the missing tokens in the wikitext103 learn better weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7D_33VlIEJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=1e-3\n",
        "lrs = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4tho1agIFK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2bf2mlNIO_M",
        "colab_type": "text"
      },
      "source": [
        "Note that we print out accuracy and keep track of how often we end up predicting the target word correctly. While this is a good metric to check, it is not part of our loss function as it can get quite bumpy. We only minimize cross-entropy loss in the LM.\n",
        "\n",
        "The exponent of the cross-entropy loss is called the perplexity of the LM. (low perplexity is better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGv-pzgFIOSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('lm_last_ft')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhX4p-E9IRa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.load('lm_last_ft')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyVRE_meIScS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL5qSH_EIUe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BUx04wof0PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.sched.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR28GFs9f37g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyQ-BNGzf4D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('lm1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mzy-uSUf4IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save_encoder('lm1_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UibR8vZZf4CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.sched.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eF63AoQf3_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
        "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvMstOFgAXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_trn, trn_labels = get_all(df_trn, 1)\n",
        "tok_val, val_labels = get_all(df_val, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmDdiYSPgA4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
        "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
        "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-jRWnssgA8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
        "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1p6dryPgBDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
        "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
        "len(itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk0ERktygBHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
        "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
        "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUy14l5GgBLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
        "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
        "\n",
        "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
        "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
        "\n",
        "bptt,em_sz,nh,nl = 70,400,1150,3\n",
        "vs = len(itos)\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "bs = 48\n",
        "\n",
        "min_lbl = trn_labels.min()\n",
        "trn_labels -= min_lbl\n",
        "val_labels -= min_lbl\n",
        "c=int(trn_labels.max())+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytptDu6agBg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_ds = TextDataset(trn_clas, trn_labels)\n",
        "val_ds = TextDataset(val_clas, val_labels)\n",
        "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
        "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
        "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
        "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
        "md = ModelData(PATH, trn_dl, val_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ditsscQgBAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# part 1\n",
        "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
        "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
        "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
        "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
        "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
        "\n",
        "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
        "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "learn.clip=.25\n",
        "learn.metrics = [accuracy]\n",
        "\n",
        "lr=3e-3\n",
        "lrm = 2.6\n",
        "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
        "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
        "wd = 1e-7\n",
        "wd = 0\n",
        "learn.load_encoder('lm1_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkdYJKjGgen_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-1)\n",
        "learn.lr_find(lrs/1000)\n",
        "learn.sched.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkG9gN70ggaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ996Q2ogh8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('clas_0')\n",
        "learn.load('clas_0')\n",
        "learn.freeze_to(-2)\n",
        "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrsJl3ApgmMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('clas_1')\n",
        "learn.load('clas_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hROECMQdgmQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ua0f1iNgqVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.sched.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgWMYutJgr77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('clas_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO1bOHJXgvjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.sched.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}