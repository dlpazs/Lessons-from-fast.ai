{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson3and4Fastai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/cnn-fastai/blob/master/Lesson3and4Fastai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkOm_qJmngRd",
        "colab_type": "text"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "precompute=True : Pre-compute the activations that come out of the last convolutional layer. Remember, **activation is a number that is calculated based on some weights/parameter that makes up kernels/filters, and they get applied to the previous layer’s activations or inputs.**\n",
        "\n",
        "learn \n",
        "\n",
        "Sequential(\n",
        "\n",
        "  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(1): Dropout(**p=0.5**)\n",
        "\n",
        "(2): Linear(in_features=1024, out_features=512)\n",
        "\n",
        "(3): ReLU()\n",
        "\n",
        "(4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(5): Dropout(p=0.5)\n",
        "\n",
        "(6): Linear(in_features=512, out_features=120)\n",
        "\n",
        "(7): LogSoftmax()\n",
        "\n",
        ")\n",
        "\n",
        "learn — This will display the layers we added at the end. These are the layers we train when precompute=True\n",
        "\n",
        "(0), (4): BatchNorm will be covered in the last lesson\n",
        "\n",
        "(1), (5): Dropout\n",
        "\n",
        "(2):**Linear layer simply means a matrix multiply**. This is a matrix which has **1024 rows and 512 columns, so it will take in 1024 activations and spit out 512 activations.**\n",
        "\n",
        "(3):ReLU — just replace negatives with zero\n",
        "\n",
        "(6): Linear — the second linear layer that takes those 512 activations from the previous linear layer and put them through a new matrix multiply 512 by 120 and outputs 120 activations\n",
        "\n",
        "(7): Softmax — The activation function that returns numbers that adds up to 1 and each of them is between 0 and 1:\n",
        "\n",
        "To calculate this we do e to the power of an activation then divide that by the e to the power of all the other activations\n",
        "\n",
        "For minor numerical precision reasons, it turns out to be better to tahe the log of the softmax than softmax directly [15:03]. That is why when we get predictions out of our models, we have to do np.exp(log_preds).\n",
        "\n",
        "What is Dropout and what is p? [08:17]\n",
        "Dropout(p=0.5)\n",
        "\n",
        "**If we applied dropout with p=0.5 to Conv2 layer, it would look like the above. We go through, pick an activation, and delete it with 50% chance.** So p=0.5 is the probability of deleting that cell. Output does not actually change by very much, just a little bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHRRDYjoiW9",
        "colab_type": "text"
      },
      "source": [
        "**It forces it to not overfit.** In other words, when a particular activation that learned just that exact dog or exact cat gets dropped out, the model has to try and **find a representation that continues to work even as random half of the activations get thrown away every time.**\n",
        "\n",
        "This has been absolutely critical in making modern deep learning work and just about **solve the problem of generalization.** Geoffrey Hinton and his colleagues came up with this idea loosely inspired by the way the brain works.\n",
        "\n",
        "Have you wondered why the validation losses better than the training losses particularly early in the training? [12:32] **This is because we turn off dropout when we run inference (i.e. making prediction) on the validation set. We want to be using the best model we can.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Xb0_ZNpJmq",
        "colab_type": "text"
      },
      "source": [
        "We do not, but PyTorch does two things when you say p=0.5. It** throws away half of the activations, and it doubles all the activations that are already there so that average activation does not change.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP-5mvLzpR0U",
        "colab_type": "text"
      },
      "source": [
        "**In Fast.ai, you can pass in ps which is the p value for all of the added layers.** It will not change the dropout in the pre-trained network since it should have been already trained with some appropriate level of dropout:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYq-jlLupXGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = ConvLearner.pretrained(arch, data, ps=0.5, precompute=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhhZt_K7pXv-",
        "colab_type": "text"
      },
      "source": [
        "You may have noticed, it has been adding two Linear layers [16:19]. We do not have to do that. There is xtra_fc parameter you can set. Note: you do need at least one which takes the output of the convolutional layer (4096 in this example) and turns it into the number of classes (120 dog breeds):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-gMzZIJptKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, \n",
        "            xtra_fc=[]); learn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZIgIS6Ypvzg",
        "colab_type": "text"
      },
      "source": [
        "learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, \n",
        "            **xtra_fc=[]**); \n",
        "            \n",
        "            learn \n",
        "\n",
        "Sequential(\n",
        "\n",
        "  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(1): Linear(in_features=1024, out_features=120)\n",
        "\n",
        "(2): LogSoftmax()\n",
        "\n",
        ")\n",
        "\n",
        "learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, \n",
        "            **xtra_fc=[700, 300])**; \n",
        "            \n",
        "            learn\n",
        "            \n",
        "            \n",
        "Sequential(\n",
        "\n",
        "  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(1): Linear(in_features=1024, out_features=**700**)\n",
        "\n",
        "(2): ReLU()\n",
        "\n",
        "(3): BatchNorm1d(700, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(4): Linear(in_features=700, out_features=**300**)\n",
        "\n",
        "(5): ReLU()\n",
        "\n",
        "(6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True)\n",
        "\n",
        "(7): Linear(in_features=300, out_features=120)\n",
        "\n",
        "(8): LogSoftmax()\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idvg8Nb5qRl5",
        "colab_type": "text"
      },
      "source": [
        "Question: Is there a particular way in which you can determine if it is overfitted? [19:53]. Yes, you can see the **training loss is much lower than the validation loss. **You cannot tell if it is too overfitted. Zero overfitting is not generally optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X8juKG9qk_T",
        "colab_type": "text"
      },
      "source": [
        "If in doubt, use the same dropout for every fully connected layer.\n",
        "There is no intution for setting different ps for earlier/later layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY9ZBUfV9cjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD54sC-r9pc4",
        "colab_type": "code",
        "outputId": "967dd65d-dbaa-45d6-ea70-e01500644cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        }
      },
      "source": [
        "!pip install fastai==0.7.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai==0.7.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/6d/9d0d6e17a78b0598d5e8c49a0d03ffc7ff265ae62eca3e2345fab14edb9b/fastai-0.7.0-py3-none-any.whl (112kB)\n",
            "\r\u001b[K    9% |███                             | 10kB 19.8MB/s eta 0:00:01\r\u001b[K    18% |█████▉                          | 20kB 2.8MB/s eta 0:00:01\r\u001b[K    27% |████████▊                       | 30kB 3.3MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    45% |██████████████▌                 | 51kB 3.3MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 61kB 3.9MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 71kB 4.0MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 81kB 3.9MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 92kB 4.3MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 102kB 4.4MB/s eta 0:00:01\r\u001b[K    99% |████████████████████████████████| 112kB 4.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 122kB 5.8MB/s \n",
            "\u001b[?25hCollecting sklearn-pandas (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9c/c94f46b40b86d2c77c46c4c1b858fc66c117b4390665eca28f2e0812db45/sklearn_pandas-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.6.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.5.3)\n",
            "Collecting pandas-summary (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/97/55/ea54109a4e7a8e7342bdf23e9382c858224263d984b0d95610568e564f59/pandas_summary-0.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: simplegeneric in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.8.1)\n",
            "Collecting widgetsnbextension (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/81/35789a3952afb48238289171728072d26d6e76649ddc8b3588657a2d78c1/widgetsnbextension-3.4.2-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.2.3)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.10.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.19.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.3.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (3.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2018.8.24)\n",
            "Collecting graphviz (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (1.14.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (3.4.3.18)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.1.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (3.0.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.7.5)\n",
            "Collecting plotnine (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/49/7af942bc63277dfca6ad397415f4cff60789c56d173b1f7edf0bd30e27e0/plotnine-0.4.0-py2.py3-none-any.whl (3.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.6MB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.1.7)\n",
            "Collecting jedi (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/1a/9bd24a185873b998611c2d8d4fb15cd5e8a879ead36355df7ee53e9111bf/jedi-0.13.1-py2.py3-none-any.whl (177kB)\n",
            "\u001b[K    100% |████████████████████████████████| 184kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.5.3)\n",
            "Collecting jupyter (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
            "Collecting torch<0.4 (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a5/e8b50b55b1abac9f1e3346c4242f1e42a82d368a8442cbd50c532922f6c4/torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 496.4MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.0.0)\n",
            "Collecting ipywidgets (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/9a/a008c7b1183fac9e52066d80a379b3c64eab535bd9d86cdc29a0b766fd82/ipywidgets-7.4.2-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 29.7MB/s \n",
            "\u001b[?25hCollecting torchvision (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.5.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.7.1)\n",
            "Collecting html5lib (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 31.0MB/s \n",
            "\u001b[?25hCollecting bcolz (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.22.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.4.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (5.5.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.10)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (4.26.0)\n",
            "Collecting isoweek (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2.6.0)\n",
            "Collecting torchtext (from fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/bc/b28b9efb4653c03e597ed207264eea45862b5260f48e9f010b5068d64db1/torchtext-0.3.1-py3-none-any.whl (62kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (2018.5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (16.0.4)\n",
            "Collecting feather-format (from fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/08/55/940b97cc6f19a19f5dab9efef2f68a0ce43a7632f858b272391f0b851a7e/feather-format-0.4.0.tar.gz\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.*) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas->fastai==0.7.*) (0.19.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension->fastai==0.7.*) (5.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->fastai==0.7.*) (1.11.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->fastai==0.7.*) (5.2.3)\n",
            "Collecting descartes>=1.1.0 (from plotnine->fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.*) (0.8.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.*) (0.5.0)\n",
            "Collecting geopandas>=0.3.0 (from plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/11/d77c157c16909bd77557d00798b05a5b6615ed60acb5900fbe6a65d35e93/geopandas-0.4.0-py2.py3-none-any.whl (899kB)\n",
            "\u001b[K    100% |████████████████████████████████| 901kB 4.4MB/s \n",
            "\u001b[?25hCollecting mizani>=0.4.5 (from plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/f3/02ea204fa75892231a008890b72ce2d9ebdb1a8eef902b38a1e0aa7e1e66/mizani-0.5.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.6MB/s \n",
            "\u001b[?25hCollecting parso>=0.3.0 (from jedi->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/51/9c48a46334be50c13d25a3afe55fa05c445699304c5ad32619de953a2305/parso-0.3.1-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 24.7MB/s \n",
            "\u001b[?25hCollecting qtconsole (from jupyter->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/22/0d8474f78a8c421d485ac2339de7c871d535160f09f170de90c8185b87c4/qtconsole-4.4.2-py2.py3-none-any.whl (112kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.*) (5.4.0)\n",
            "Collecting jupyter-console (from jupyter->fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/6374ae8c21b7d0847f9c3722dcdfac986b8e54fa9ad9ea66e1eb6320d2b8/jupyter_console-6.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai==0.7.*) (0.46)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->fastai==0.7.*) (4.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.*) (39.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.*) (1.0.15)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.*) (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->fastai==0.7.*) (2.18.4)\n",
            "Collecting pyarrow>=0.4.0 (from feather-format->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/5cc76d6ec4918f70af89316940f36291d3666ae13aad1f19660b071ff2dc/pyarrow-0.11.0-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.6MB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.*) (0.8.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.*) (4.4.0)\n",
            "Collecting pyproj (from geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/72/5c1888c4948a0c7b736d10e0f0f69966e7c0874a660222ed0a2c2c6daa9f/pyproj-1.9.5.1.tar.gz (4.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.4MB 4.3MB/s \n",
            "\u001b[?25hCollecting fiona (from geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/bf/029958f4e3811ce7017fb5805d5203e8bde6c1816b902964acb2dec67863/Fiona-1.7.13-cp36-cp36m-manylinux1_x86_64.whl (15.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 15.8MB 1.9MB/s \n",
            "\u001b[?25hCollecting shapely (from geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/b6/b53f19062afd49bb5abd049aeed36f13bf8d57ef8f3fa07a5203531a0252/Shapely-1.6.4.post2-cp36-cp36m-manylinux1_x86_64.whl (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 8.1MB/s \n",
            "\u001b[?25hCollecting palettable (from mizani>=0.4.5->plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/8a/84537c0354f0d1f03bf644b71bf8e0a50db9c1294181905721a5f3efbf66/palettable-3.1.1-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 21.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.*) (1.4.2)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.*) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.*) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.*) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.*) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.*) (3.0.4)\n",
            "Collecting munch (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/68/f4/260ec98ea840757a0da09e0ed8135333d59b8dfebe9752a365b04857660a/munch-2.3.2.tar.gz\n",
            "Collecting click-plugins (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/dd/fef84cf1678418f241ef542c0288bdf215bdd3e35f1fe03dc5223a2e80ba/click_plugins-1.0.4-py2.py3-none-any.whl\n",
            "Collecting cligj>=0.4 (from fiona->geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Collecting click>=3.0 (from click-plugins->fiona->geopandas>=0.3.0->plotnine->fastai==0.7.*)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 25.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: bcolz, feather-format, pyproj, munch\n",
            "  Running setup.py bdist_wheel for bcolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "  Running setup.py bdist_wheel for feather-format ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/85/7d/12/2dfa5c0195f921ac935f5e8f27deada74972edc0ae9988a9c1\n",
            "  Running setup.py bdist_wheel for pyproj ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/89/77/ec/a537585d1022dafde0317dd19d33c4a30d4ee61e19f25ebd8e\n",
            "  Running setup.py bdist_wheel for munch ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/db/bf/bc/06a3e1bfe0ab27d2e720ceb3cff3159398d92644c0cec2c125\n",
            "Successfully built bcolz feather-format pyproj munch\n",
            "\u001b[31mmizani 0.5.0 has requirement pandas>=0.23.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mplotnine 0.4.0 has requirement scipy>=1.0.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mjupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.15 which is incompatible.\u001b[0m\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sklearn-pandas, pandas-summary, widgetsnbextension, graphviz, descartes, pyproj, munch, click, click-plugins, cligj, fiona, shapely, geopandas, palettable, mizani, plotnine, parso, jedi, ipywidgets, qtconsole, jupyter-console, jupyter, torch, torchvision, html5lib, bcolz, isoweek, torchtext, pyarrow, feather-format, fastai\n",
            "Successfully installed bcolz-1.2.1 click-7.0 click-plugins-1.0.4 cligj-0.5.0 descartes-1.1.0 fastai-0.7.0 feather-format-0.4.0 fiona-1.7.13 geopandas-0.4.0 graphviz-0.9 html5lib-1.0.1 ipywidgets-7.4.2 isoweek-1.3.3 jedi-0.13.1 jupyter-1.0.0 jupyter-console-6.0.0 mizani-0.5.0 munch-2.3.2 palettable-3.1.1 pandas-summary-0.0.5 parso-0.3.1 plotnine-0.4.0 pyarrow-0.11.0 pyproj-1.9.5.1 qtconsole-4.4.2 shapely-1.6.4.post2 sklearn-pandas-1.7.0 torch-0.3.1 torchtext-0.3.1 torchvision-0.2.1 widgetsnbextension-3.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cwisyTW-pv4",
        "colab_type": "code",
        "outputId": "faac0247-a843-4bb2-e5e9-9cfeea1d83ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "!pip install torchtext==0.2.3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2018.8.24)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2.6)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwsZHX6S9kEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from fastai.structured import *\n",
        "from fastai.column_data import *\n",
        "np.set_printoptions(threshold=50, edgeitems=20)\n",
        "\n",
        "PATH='./'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE-vLBaJ9onc",
        "colab_type": "code",
        "outputId": "9ffca2e3-84f1-40e3-9a2f-444f9d151ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://files.fast.ai/part2/lesson14/rossmann.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-14 08:07:26--  http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7730448 (7.4M) [text/plain]\n",
            "Saving to: ‘rossmann.tgz’\n",
            "\n",
            "rossmann.tgz        100%[===================>]   7.37M  11.6MB/s    in 0.6s    \n",
            "\n",
            "2018-10-14 08:07:26 (11.6 MB/s) - ‘rossmann.tgz’ saved [7730448/7730448]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9mpCeh0-HIn",
        "colab_type": "code",
        "outputId": "d0161637-eb3b-40e5-91c0-934c3e7ec92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!tar xvfz rossmann.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "googletrend.csv\n",
            "sample_submission.csv\n",
            "state_names.csv\n",
            "store.csv\n",
            "store_states.csv\n",
            "test.csv\n",
            "train.csv\n",
            "weather.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2pEz8P3-uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat_csvs(dirname):\n",
        "    path = f'{PATH}{dirname}'\n",
        "    filenames=glob(f\"{PATH}/*.csv\")\n",
        "\n",
        "    wrote_header = False\n",
        "    with open(f\"{path}.csv\",\"w\") as outputfile:\n",
        "        for filename in filenames:\n",
        "            name = filename.split(\".\")[0]\n",
        "            with open(filename) as f:\n",
        "                line = f.readline()\n",
        "                if not wrote_header:\n",
        "                    wrote_header = True\n",
        "                    outputfile.write(\"file,\"+line)\n",
        "                for line in f:\n",
        "                     outputfile.write(name + \",\" + line)\n",
        "                outputfile.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN5ordps_B0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concat_csvs('googletrend')\n",
        "# concat_csvs('weather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRVVdVKK_SHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_names = ['train', 'store', 'store_states', 'state_names', \n",
        "               'googletrend', 'weather', 'test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqoWEBrD_Ytl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hI9biAU_Z8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML, display\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HK0wMg8qut3",
        "colab_type": "text"
      },
      "source": [
        "There are two types of columns:\n",
        "\n",
        "- Categorical — It has a number of “levels” e.g. StoreType, Assortment\n",
        "- Continuous — It has a number where differences or ratios of that numbers have some kind of meanings e.g. CompetitionDistance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VndnderL_bYm",
        "colab_type": "code",
        "outputId": "cb204235-695f-451a-9b7c-b4a09542fe26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1446
        }
      },
      "source": [
        "for t in tables: display(t.head())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>5263</td>\n",
              "      <td>555</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>6064</td>\n",
              "      <td>625</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>8314</td>\n",
              "      <td>821</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>13995</td>\n",
              "      <td>1498</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>4822</td>\n",
              "      <td>559</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
              "0      1          5  2015-07-31   5263        555     1      1            0   \n",
              "1      2          5  2015-07-31   6064        625     1      1            0   \n",
              "2      3          5  2015-07-31   8314        821     1      1            0   \n",
              "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
              "4      5          5  2015-07-31   4822        559     1      1            0   \n",
              "\n",
              "   SchoolHoliday  \n",
              "0              1  \n",
              "1              1  \n",
              "2              1  \n",
              "3              1  \n",
              "4              1  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <th>Promo2</th>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <th>PromoInterval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>1270.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>570.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>14130.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>620.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>29910.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
              "0      1         c          a               1270.0                        9.0   \n",
              "1      2         a          a                570.0                       11.0   \n",
              "2      3         a          a              14130.0                       12.0   \n",
              "3      4         c          c                620.0                        9.0   \n",
              "4      5         a          a              29910.0                        4.0   \n",
              "\n",
              "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
              "0                    2008.0       0              NaN              NaN   \n",
              "1                    2007.0       1             13.0           2010.0   \n",
              "2                    2006.0       1             14.0           2011.0   \n",
              "3                    2009.0       0              NaN              NaN   \n",
              "4                    2015.0       0              NaN              NaN   \n",
              "\n",
              "     PromoInterval  \n",
              "0              NaN  \n",
              "1  Jan,Apr,Jul,Oct  \n",
              "2  Jan,Apr,Jul,Oct  \n",
              "3              NaN  \n",
              "4              NaN  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>State</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>HE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>TH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>BE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Store State\n",
              "0      1    HE\n",
              "1      2    TH\n",
              "2      3    NW\n",
              "3      4    BE\n",
              "4      5    SN"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StateName</th>\n",
              "      <th>State</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BadenWuerttemberg</td>\n",
              "      <td>BW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bayern</td>\n",
              "      <td>BY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Berlin</td>\n",
              "      <td>BE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>BB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bremen</td>\n",
              "      <td>HB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           StateName State\n",
              "0  BadenWuerttemberg    BW\n",
              "1             Bayern    BY\n",
              "2             Berlin    BE\n",
              "3        Brandenburg    BB\n",
              "4             Bremen    HB"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>week</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "      <td>2012-12-02 - 2012-12-08</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "      <td>2012-12-09 - 2012-12-15</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "      <td>2012-12-16 - 2012-12-22</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "      <td>2012-12-23 - 2012-12-29</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "      <td>2012-12-30 - 2013-01-05</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             file                     week  trend\n",
              "0  Rossmann_DE_SN  2012-12-02 - 2012-12-08     96\n",
              "1  Rossmann_DE_SN  2012-12-09 - 2012-12-15     95\n",
              "2  Rossmann_DE_SN  2012-12-16 - 2012-12-22     91\n",
              "3  Rossmann_DE_SN  2012-12-23 - 2012-12-29     48\n",
              "4  Rossmann_DE_SN  2012-12-30 - 2013-01-05     67"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>Date</th>\n",
              "      <th>Max_TemperatureC</th>\n",
              "      <th>Mean_TemperatureC</th>\n",
              "      <th>Min_TemperatureC</th>\n",
              "      <th>Dew_PointC</th>\n",
              "      <th>MeanDew_PointC</th>\n",
              "      <th>Min_DewpointC</th>\n",
              "      <th>Max_Humidity</th>\n",
              "      <th>Mean_Humidity</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_VisibilityKm</th>\n",
              "      <th>Mean_VisibilityKm</th>\n",
              "      <th>Min_VisibilitykM</th>\n",
              "      <th>Max_Wind_SpeedKm_h</th>\n",
              "      <th>Mean_Wind_SpeedKm_h</th>\n",
              "      <th>Max_Gust_SpeedKm_h</th>\n",
              "      <th>Precipitationmm</th>\n",
              "      <th>CloudCover</th>\n",
              "      <th>Events</th>\n",
              "      <th>WindDirDegrees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NordrheinWestfalen</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>87</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>39</td>\n",
              "      <td>26</td>\n",
              "      <td>58.0</td>\n",
              "      <td>5.08</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Rain</td>\n",
              "      <td>215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NordrheinWestfalen</td>\n",
              "      <td>2013-01-02</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>85</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Rain</td>\n",
              "      <td>225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NordrheinWestfalen</td>\n",
              "      <td>2013-01-03</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>93</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>26</td>\n",
              "      <td>21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.02</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Rain</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NordrheinWestfalen</td>\n",
              "      <td>2013-01-04</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>100</td>\n",
              "      <td>94</td>\n",
              "      <td>...</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>23</td>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Rain</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NordrheinWestfalen</td>\n",
              "      <td>2013-01-05</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>100</td>\n",
              "      <td>94</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Rain</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 file        Date  Max_TemperatureC  Mean_TemperatureC  \\\n",
              "0  NordrheinWestfalen  2013-01-01                 8                  4   \n",
              "1  NordrheinWestfalen  2013-01-02                 7                  4   \n",
              "2  NordrheinWestfalen  2013-01-03                11                  8   \n",
              "3  NordrheinWestfalen  2013-01-04                 9                  9   \n",
              "4  NordrheinWestfalen  2013-01-05                 8                  8   \n",
              "\n",
              "   Min_TemperatureC  Dew_PointC  MeanDew_PointC  Min_DewpointC  Max_Humidity  \\\n",
              "0                 2           7               5              1            94   \n",
              "1                 1           5               3              2            93   \n",
              "2                 6          10               8              4           100   \n",
              "3                 8           9               9              8           100   \n",
              "4                 7           8               7              6           100   \n",
              "\n",
              "   Mean_Humidity       ...        Max_VisibilityKm  Mean_VisibilityKm  \\\n",
              "0             87       ...                    31.0               12.0   \n",
              "1             85       ...                    31.0               14.0   \n",
              "2             93       ...                    31.0                8.0   \n",
              "3             94       ...                    11.0                5.0   \n",
              "4             94       ...                    10.0                6.0   \n",
              "\n",
              "   Min_VisibilitykM  Max_Wind_SpeedKm_h  Mean_Wind_SpeedKm_h  \\\n",
              "0               4.0                  39                   26   \n",
              "1              10.0                  24                   16   \n",
              "2               2.0                  26                   21   \n",
              "3               2.0                  23                   14   \n",
              "4               3.0                  16                   10   \n",
              "\n",
              "   Max_Gust_SpeedKm_h  Precipitationmm  CloudCover  Events  WindDirDegrees  \n",
              "0                58.0             5.08         6.0    Rain             215  \n",
              "1                 NaN             0.00         6.0    Rain             225  \n",
              "2                 NaN             1.02         7.0    Rain             240  \n",
              "3                 NaN             0.25         7.0    Rain             263  \n",
              "4                 NaN             0.00         7.0    Rain             268  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  Store  DayOfWeek        Date  Open  Promo StateHoliday  SchoolHoliday\n",
              "0   1      1          4  2015-09-17   1.0      1            0              0\n",
              "1   2      3          4  2015-09-17   1.0      1            0              0\n",
              "2   3      7          4  2015-09-17   1.0      1            0              0\n",
              "3   4      8          4  2015-09-17   1.0      1            0              0\n",
              "4   5      9          4  2015-09-17   1.0      1            0              0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb_-dJR5_hUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, store, store_states, state_names, googletrend, weather, test = tables\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paYnJx6rACOr",
        "colab_type": "code",
        "outputId": "d03527a1-cd78-41e8-d02e-eaa63c929dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train),len(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1017209, 41088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsvZLGx6AEZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.StateHoliday = train.StateHoliday!='0'\n",
        "test.StateHoliday = test.StateHoliday!='0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mIyFfgBAG2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_df(left, right, left_on, right_on=None, suffix='_y'):\n",
        "    if right_on is None: right_on = left_on\n",
        "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
        "                      suffixes=(\"\", suffix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nENXLIzTALm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather = join_df(weather, state_names, \"file\", \"StateName\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtfY1aJbAUdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\n",
        "googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n",
        "googletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XcLKnPM15yd",
        "colab_type": "text"
      },
      "source": [
        "There is a Fast.ai function called add_datepart which takes a data frame and a column name. It optionally removes the column from the data frame and replaces it with lots of column representing all of the useful information about that date such as day of week, day of month, month of year, etc (basically everything Pandas gives us)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_t6m6iCAXP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_datepart(weather, \"Date\", drop=False)\n",
        "add_datepart(googletrend, \"Date\", drop=False)\n",
        "add_datepart(train, \"Date\", drop=False)\n",
        "add_datepart(test, \"Date\", drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qVyxu3CAeAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trend_de = googletrend[googletrend.file == 'Rossmann_DE']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFG-HWiVAm9N",
        "colab_type": "code",
        "outputId": "40e2ecf1-394c-4d48-fff4-e5970d0fe14b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "store = join_df(store, store_states, \"Store\")\n",
        "len(store[store.State.isnull()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIRJfJCNAoas",
        "colab_type": "code",
        "outputId": "132eb7fb-bc86-45fa-c63f-9ab228e881dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "joined = join_df(train, store, \"Store\")\n",
        "joined_test = join_df(test, store, \"Store\")\n",
        "len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5xJMtcCAqO8",
        "colab_type": "code",
        "outputId": "d9ac4151-680a-401c-941d-b76f8dc6465d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "joined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3TbQL4AAvr1",
        "colab_type": "code",
        "outputId": "6fc728f2-85ed-472b-f5f3-62e5edf5e09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "joined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWj7piQWAxPN",
        "colab_type": "code",
        "outputId": "ec609a79-bbd5-4d00-837a-d8f0511d4bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "joined = join_df(joined, weather, [\"State\",\"Date\"])\n",
        "joined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\n",
        "len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBwrpae4AzFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined, joined_test):\n",
        "    for c in df.columns:\n",
        "        if c.endswith('_y'):\n",
        "            if c in df.columns: df.drop(c, inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznM8X7-A0UF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for df in (joined,joined_test):\n",
        "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
        "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
        "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
        "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lep2Ngz6A2EH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n",
        "                                                     month=df.CompetitionOpenSinceMonth, day=15))\n",
        "    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf1RSrQAA3pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
        "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq87fhvgA5L5",
        "colab_type": "code",
        "outputId": "0b243463-b403-411a-8ea1-6106283abf40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
        "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n",
        "joined.CompetitionMonthsOpen.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24,  3, 19,  9,  0, 16, 17,  7, 15, 22, 11, 13,  2, 23, 12,  4, 10,  1, 14, 20,  8, 18,  6, 21,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5aNeVWA6sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for df in (joined,joined_test):\n",
        "    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n",
        "        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime))\n",
        "    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJECLZ9nA8Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
        "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
        "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
        "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
        "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
        "    df.Promo2Weeks.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt0OgFq5A9dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.to_feather(f'{PATH}joined')\n",
        "joined_test.to_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P66VRBuA-9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_elapsed(fld, pre):\n",
        "    day1 = np.timedelta64(1, 'D')\n",
        "    last_date = np.datetime64()\n",
        "    last_store = 0\n",
        "    res = []\n",
        "\n",
        "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
        "        if s != last_store:\n",
        "            last_date = np.datetime64()\n",
        "            last_store = s\n",
        "        if v: last_date = d\n",
        "        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n",
        "    df[pre+fld] = res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JoQXPYdBCve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzHd4shsBD6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = train[columns].append(test[columns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAkeFA5ABFJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'SchoolHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrjzARAEBGde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'StateHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nDfAsfgBHkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'Promo'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B80ZX2XRBJSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.set_index(\"Date\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L9_BVUjBKoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['SchoolHoliday', 'StateHoliday', 'Promo']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvJOeunvBLvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for o in ['Before', 'After']:\n",
        "    for p in columns:\n",
        "        a = o+p\n",
        "        df[a] = df[a].fillna(0).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FinmVnMfBNEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAnhnQ86BO5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fwd = df[['Store']+columns].sort_index(ascending=False\n",
        "                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur74IG4mBQsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwd.drop('Store',1,inplace=True)\n",
        "bwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js8NfaVurqjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fwd.drop('Store',1,inplace=True)\n",
        "fwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGqTeJaarsDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.reset_index(inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8F7taBQrtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\n",
        "df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-sMOGferury",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns,1,inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2sM1uqrv0B",
        "colab_type": "code",
        "outputId": "0264a220-5c36-4d34-9677-739a12df25ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Store</th>\n",
              "      <th>AfterSchoolHoliday</th>\n",
              "      <th>BeforeSchoolHoliday</th>\n",
              "      <th>AfterStateHoliday</th>\n",
              "      <th>BeforeStateHoliday</th>\n",
              "      <th>AfterPromo</th>\n",
              "      <th>BeforePromo</th>\n",
              "      <th>SchoolHoliday_bw</th>\n",
              "      <th>StateHoliday_bw</th>\n",
              "      <th>Promo_bw</th>\n",
              "      <th>SchoolHoliday_fw</th>\n",
              "      <th>StateHoliday_fw</th>\n",
              "      <th>Promo_fw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-09-16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-09-15</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-09-14</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-09-13</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Store  AfterSchoolHoliday  BeforeSchoolHoliday  \\\n",
              "0 2015-09-17      1                  13                    0   \n",
              "1 2015-09-16      1                  12                    0   \n",
              "2 2015-09-15      1                  11                    0   \n",
              "3 2015-09-14      1                  10                    0   \n",
              "4 2015-09-13      1                   9                    0   \n",
              "\n",
              "   AfterStateHoliday  BeforeStateHoliday  AfterPromo  BeforePromo  \\\n",
              "0                105                   0           0            0   \n",
              "1                104                   0           0            0   \n",
              "2                103                   0           0            0   \n",
              "3                102                   0           0            0   \n",
              "4                101                   0           9           -1   \n",
              "\n",
              "   SchoolHoliday_bw  StateHoliday_bw  Promo_bw  SchoolHoliday_fw  \\\n",
              "0               0.0              0.0       4.0               0.0   \n",
              "1               0.0              0.0       3.0               0.0   \n",
              "2               0.0              0.0       2.0               0.0   \n",
              "3               0.0              0.0       1.0               0.0   \n",
              "4               0.0              0.0       0.0               0.0   \n",
              "\n",
              "   StateHoliday_fw  Promo_fw  \n",
              "0              0.0       1.0  \n",
              "1              0.0       2.0  \n",
              "2              0.0       3.0  \n",
              "3              0.0       4.0  \n",
              "4              0.0       4.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH2ZJS_Frwyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_feather(f'{PATH}df')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ2mynwyryQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_feather(f'{PATH}df')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-HqOSYGrzYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Date\"] = pd.to_datetime(df.Date)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfwlClfir3C5",
        "colab_type": "code",
        "outputId": "816f3d66-0a76-44ea-8b96-831435f093f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'Store', 'AfterSchoolHoliday', 'BeforeSchoolHoliday',\n",
              "       'AfterStateHoliday', 'BeforeStateHoliday', 'AfterPromo', 'BeforePromo',\n",
              "       'SchoolHoliday_bw', 'StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw',\n",
              "       'StateHoliday_fw', 'Promo_fw'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wAfM5Rwr6KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = join_df(joined, df, ['Store', 'Date'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ4QE3OQr72O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined_test = join_df(joined_test, df, ['Store', 'Date'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vkGLyfGr9e7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = joined[joined.Sales!=0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixeSGfRVr_F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.reset_index(inplace=True)\n",
        "joined_test.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foNVWKdXsAZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.to_feather(f'{PATH}joined')\n",
        "joined_test.to_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxXuYa77sBnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = pd.read_feather(f'{PATH}joined')\n",
        "joined_test = pd.read_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNg319OhsGk5",
        "colab_type": "code",
        "outputId": "931e7d6f-811c-4cdf-b9c9-231c56430c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1345
        }
      },
      "source": [
        "joined.head().T.head(40)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Store</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DayOfWeek</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sales</th>\n",
              "      <td>5263</td>\n",
              "      <td>6064</td>\n",
              "      <td>8314</td>\n",
              "      <td>13995</td>\n",
              "      <td>4822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Customers</th>\n",
              "      <td>555</td>\n",
              "      <td>625</td>\n",
              "      <td>821</td>\n",
              "      <td>1498</td>\n",
              "      <td>559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Open</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Month</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Week</th>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Day</th>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dayofweek</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dayofyear</th>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_month_end</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_month_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_quarter_end</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_quarter_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_year_end</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_year_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elapsed</th>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StoreType</th>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Assortment</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <td>1270</td>\n",
              "      <td>570</td>\n",
              "      <td>14130</td>\n",
              "      <td>620</td>\n",
              "      <td>29910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <td>2008</td>\n",
              "      <td>2007</td>\n",
              "      <td>2006</td>\n",
              "      <td>2009</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2SinceYear</th>\n",
              "      <td>1900</td>\n",
              "      <td>2010</td>\n",
              "      <td>2011</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PromoInterval</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <td>HE</td>\n",
              "      <td>TH</td>\n",
              "      <td>NW</td>\n",
              "      <td>BE</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>file</th>\n",
              "      <td>Rossmann_DE_HE</td>\n",
              "      <td>Rossmann_DE_TH</td>\n",
              "      <td>Rossmann_DE_NW</td>\n",
              "      <td>Rossmann_DE_BE</td>\n",
              "      <td>Rossmann_DE_SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>week</th>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trend</th>\n",
              "      <td>85</td>\n",
              "      <td>80</td>\n",
              "      <td>86</td>\n",
              "      <td>74</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>file_DE</th>\n",
              "      <td>Rossmann_DE</td>\n",
              "      <td>Rossmann_DE</td>\n",
              "      <td>Rossmann_DE</td>\n",
              "      <td>Rossmann_DE</td>\n",
              "      <td>Rossmann_DE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>week_DE</th>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "      <td>2015-08-02 - 2015-08-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trend_DE</th>\n",
              "      <td>83</td>\n",
              "      <td>83</td>\n",
              "      <td>83</td>\n",
              "      <td>83</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date_DE</th>\n",
              "      <td>2015-08-02 00:00:00</td>\n",
              "      <td>2015-08-02 00:00:00</td>\n",
              "      <td>2015-08-02 00:00:00</td>\n",
              "      <td>2015-08-02 00:00:00</td>\n",
              "      <td>2015-08-02 00:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 0                        1  \\\n",
              "index                                            0                        1   \n",
              "Store                                            1                        2   \n",
              "DayOfWeek                                        5                        5   \n",
              "Date                           2015-07-31 00:00:00      2015-07-31 00:00:00   \n",
              "Sales                                         5263                     6064   \n",
              "Customers                                      555                      625   \n",
              "Open                                             1                        1   \n",
              "Promo                                            1                        1   \n",
              "StateHoliday                                 False                    False   \n",
              "SchoolHoliday                                    1                        1   \n",
              "Year                                          2015                     2015   \n",
              "Month                                            7                        7   \n",
              "Week                                            31                       31   \n",
              "Day                                             31                       31   \n",
              "Dayofweek                                        4                        4   \n",
              "Dayofyear                                      212                      212   \n",
              "Is_month_end                                  True                     True   \n",
              "Is_month_start                               False                    False   \n",
              "Is_quarter_end                               False                    False   \n",
              "Is_quarter_start                             False                    False   \n",
              "Is_year_end                                  False                    False   \n",
              "Is_year_start                                False                    False   \n",
              "Elapsed                                 1438300800               1438300800   \n",
              "StoreType                                        c                        a   \n",
              "Assortment                                       a                        a   \n",
              "CompetitionDistance                           1270                      570   \n",
              "CompetitionOpenSinceMonth                        9                       11   \n",
              "CompetitionOpenSinceYear                      2008                     2007   \n",
              "Promo2                                           0                        1   \n",
              "Promo2SinceWeek                                  1                       13   \n",
              "Promo2SinceYear                               1900                     2010   \n",
              "PromoInterval                                  NaN          Jan,Apr,Jul,Oct   \n",
              "State                                           HE                       TH   \n",
              "file                                Rossmann_DE_HE           Rossmann_DE_TH   \n",
              "week                       2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
              "trend                                           85                       80   \n",
              "file_DE                                Rossmann_DE              Rossmann_DE   \n",
              "week_DE                    2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
              "trend_DE                                        83                       83   \n",
              "Date_DE                        2015-08-02 00:00:00      2015-08-02 00:00:00   \n",
              "\n",
              "                                                 2                        3  \\\n",
              "index                                            2                        3   \n",
              "Store                                            3                        4   \n",
              "DayOfWeek                                        5                        5   \n",
              "Date                           2015-07-31 00:00:00      2015-07-31 00:00:00   \n",
              "Sales                                         8314                    13995   \n",
              "Customers                                      821                     1498   \n",
              "Open                                             1                        1   \n",
              "Promo                                            1                        1   \n",
              "StateHoliday                                 False                    False   \n",
              "SchoolHoliday                                    1                        1   \n",
              "Year                                          2015                     2015   \n",
              "Month                                            7                        7   \n",
              "Week                                            31                       31   \n",
              "Day                                             31                       31   \n",
              "Dayofweek                                        4                        4   \n",
              "Dayofyear                                      212                      212   \n",
              "Is_month_end                                  True                     True   \n",
              "Is_month_start                               False                    False   \n",
              "Is_quarter_end                               False                    False   \n",
              "Is_quarter_start                             False                    False   \n",
              "Is_year_end                                  False                    False   \n",
              "Is_year_start                                False                    False   \n",
              "Elapsed                                 1438300800               1438300800   \n",
              "StoreType                                        a                        c   \n",
              "Assortment                                       a                        c   \n",
              "CompetitionDistance                          14130                      620   \n",
              "CompetitionOpenSinceMonth                       12                        9   \n",
              "CompetitionOpenSinceYear                      2006                     2009   \n",
              "Promo2                                           1                        0   \n",
              "Promo2SinceWeek                                 14                        1   \n",
              "Promo2SinceYear                               2011                     1900   \n",
              "PromoInterval                      Jan,Apr,Jul,Oct                      NaN   \n",
              "State                                           NW                       BE   \n",
              "file                                Rossmann_DE_NW           Rossmann_DE_BE   \n",
              "week                       2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
              "trend                                           86                       74   \n",
              "file_DE                                Rossmann_DE              Rossmann_DE   \n",
              "week_DE                    2015-08-02 - 2015-08-08  2015-08-02 - 2015-08-08   \n",
              "trend_DE                                        83                       83   \n",
              "Date_DE                        2015-08-02 00:00:00      2015-08-02 00:00:00   \n",
              "\n",
              "                                                 4  \n",
              "index                                            4  \n",
              "Store                                            5  \n",
              "DayOfWeek                                        5  \n",
              "Date                           2015-07-31 00:00:00  \n",
              "Sales                                         4822  \n",
              "Customers                                      559  \n",
              "Open                                             1  \n",
              "Promo                                            1  \n",
              "StateHoliday                                 False  \n",
              "SchoolHoliday                                    1  \n",
              "Year                                          2015  \n",
              "Month                                            7  \n",
              "Week                                            31  \n",
              "Day                                             31  \n",
              "Dayofweek                                        4  \n",
              "Dayofyear                                      212  \n",
              "Is_month_end                                  True  \n",
              "Is_month_start                               False  \n",
              "Is_quarter_end                               False  \n",
              "Is_quarter_start                             False  \n",
              "Is_year_end                                  False  \n",
              "Is_year_start                                False  \n",
              "Elapsed                                 1438300800  \n",
              "StoreType                                        a  \n",
              "Assortment                                       a  \n",
              "CompetitionDistance                          29910  \n",
              "CompetitionOpenSinceMonth                        4  \n",
              "CompetitionOpenSinceYear                      2015  \n",
              "Promo2                                           0  \n",
              "Promo2SinceWeek                                  1  \n",
              "Promo2SinceYear                               1900  \n",
              "PromoInterval                                  NaN  \n",
              "State                                           SN  \n",
              "file                                Rossmann_DE_SN  \n",
              "week                       2015-08-02 - 2015-08-08  \n",
              "trend                                           82  \n",
              "file_DE                                Rossmann_DE  \n",
              "week_DE                    2015-08-02 - 2015-08-08  \n",
              "trend_DE                                        83  \n",
              "Date_DE                        2015-08-02 00:00:00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCV2flRCseyZ",
        "colab_type": "text"
      },
      "source": [
        "Now that we've engineered all our features, we need to convert to input compatible with a neural network.\n",
        "\n",
        "This includes converting categorical variables into contiguous integers or one-hot encodings, normalizing continuous features to standard normal, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKjUwrgbslTh",
        "colab_type": "text"
      },
      "source": [
        "Numbers like Year , Month, although we could treat them as continuous, we do not have to. If we decide to make Year a categorical variable, we are telling our neural net that for every different “level”of Year (2000, 2001, 2002), you can treat it totally differently; where-else if we say it is continuous, it has to come up with some kind of smooth function to fit them. So often things that actually are continuous but do not have many distinct levels (e.g. Year, DayOfWeek), it often works better to treat them as categorical.\n",
        "Choosing categorical vs. continuous variable is a modeling decision you get to make. In summary, if it is categorical in the data, it has to be categorical. If it is continuous in the data, you get to pick whether to make it continuous or categorical in the model.\n",
        "Generally, floating point numbers are hard to make categorical as there are many levels (we call number of levels “Cardinality” — e.g. the cardinality of the day of week variable is 7)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNAD1DPxsTts",
        "colab_type": "code",
        "outputId": "fcfd554a-edd7-4a5c-fa18-bb4bc39bc292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
        "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
        "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
        "    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
        "\n",
        "contin_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
        "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
        "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
        "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
        "\n",
        "n = len(joined); n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "844338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnMIovzisYFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dep = 'Sales'\n",
        "joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Ls4OlJsuEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "joined_test[dep] = 0\n",
        "joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRddc9lwsvgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edWPezriswp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apply_cats(joined_test, joined)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huhvdVQLs2BN",
        "colab_type": "text"
      },
      "source": [
        "Loop through cat_vars and turn applicable data frame columns into categorical columns.\n",
        "Loop through contin_vars and set them as float32 (32 bit floating point) because that is what PyTorch expects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1t6OwWtsxsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for v in contin_vars:\n",
        "    joined[v] = joined[v].fillna(0).astype('float32')\n",
        "    joined_test[v] = joined_test[v].fillna(0).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBy350RUszFg",
        "colab_type": "code",
        "outputId": "25572640-5e05-406f-940e-39376b4f61dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Start with a small sample\n",
        "idxs = get_cv_idxs(n, val_pct=150000/n)\n",
        "joined_samp = joined.iloc[idxs].set_index(\"Date\")\n",
        "samp_size = len(joined_samp); samp_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTVt1K-s425",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samp_size = n\n",
        "joined_samp = joined.set_index(\"Date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8fYxXYLs_Ku",
        "colab_type": "code",
        "outputId": "a2874e25-3b0f-4dd6-fb6a-69d9efb7ddf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "joined_samp.head(2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>CompetitionMonthsOpen</th>\n",
              "      <th>Promo2Weeks</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Wind_SpeedKm_h</th>\n",
              "      <th>Mean_Wind_SpeedKm_h</th>\n",
              "      <th>CloudCover</th>\n",
              "      <th>trend</th>\n",
              "      <th>trend_DE</th>\n",
              "      <th>AfterStateHoliday</th>\n",
              "      <th>BeforeStateHoliday</th>\n",
              "      <th>Promo</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>False</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>...</td>\n",
              "      <td>24.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>False</td>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Store DayOfWeek  Year Month Day StateHoliday CompetitionMonthsOpen  \\\n",
              "Date                                                                            \n",
              "2015-07-31     1         5  2015     7  31        False                    24   \n",
              "2015-07-31     2         5  2015     7  31        False                    24   \n",
              "\n",
              "           Promo2Weeks StoreType Assortment  ...  Max_Wind_SpeedKm_h  \\\n",
              "Date                                         ...                       \n",
              "2015-07-31           0         c          a  ...                24.0   \n",
              "2015-07-31          25         a          a  ...                14.0   \n",
              "\n",
              "           Mean_Wind_SpeedKm_h CloudCover trend trend_DE AfterStateHoliday  \\\n",
              "Date                                                                         \n",
              "2015-07-31                11.0        1.0  85.0     83.0              57.0   \n",
              "2015-07-31                11.0        4.0  80.0     83.0              67.0   \n",
              "\n",
              "           BeforeStateHoliday Promo SchoolHoliday Sales  \n",
              "Date                                                     \n",
              "2015-07-31                0.0   1.0           1.0  5263  \n",
              "2015-07-31                0.0   1.0           1.0  6064  \n",
              "\n",
              "[2 rows x 39 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANk_NlqntRbF",
        "colab_type": "text"
      },
      "source": [
        "proc_df (process data frame) — A function in Fast.ai that does a few things:\n",
        "\n",
        "1.** Pulls out the dependent variable, puts it into a separate variable**, and **deletes it from the original data frame.** *In other words, df do not have Sales column, and y only contains Sales column.*\n",
        "2. **do_scale :** **Neural nets really like to have the input data to all be somewhere around zero with a standard deviation of somewhere around 1. So we take our data, subtract the mean, and divide by the standard deviation to make that happen.** It returns a special object which keeps track of what mean and standard deviation it used for that normalization so you can do the same to the test set later (mapper).\n",
        "\n",
        "3. It also **handles missing values — for categorical variable,** it becomes ID: 0 and other categories become 1, 2, 3, and so on. **For continuous variable, it replaces the missing value with the median and create a new boolean column that says whether it was missing or not.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMJfWdo0tApw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)\n",
        "yl = np.log(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g0EKbqBtNTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined_test = joined_test.set_index(\"Date\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwrOYg1wuKKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'],\n",
        "                                  mapper=mapper, na_dict=nas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9GddCkuL0r",
        "colab_type": "code",
        "outputId": "250decae-7b8f-49d4-ff40-1e487f446a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "df.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>CompetitionMonthsOpen</th>\n",
              "      <th>Promo2Weeks</th>\n",
              "      <th>StoreType</th>\n",
              "      <th>Assortment</th>\n",
              "      <th>...</th>\n",
              "      <th>Min_Humidity</th>\n",
              "      <th>Max_Wind_SpeedKm_h</th>\n",
              "      <th>Mean_Wind_SpeedKm_h</th>\n",
              "      <th>CloudCover</th>\n",
              "      <th>trend</th>\n",
              "      <th>trend_DE</th>\n",
              "      <th>AfterStateHoliday</th>\n",
              "      <th>BeforeStateHoliday</th>\n",
              "      <th>Promo</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.620066</td>\n",
              "      <td>0.149027</td>\n",
              "      <td>-0.142774</td>\n",
              "      <td>-1.844823</td>\n",
              "      <td>1.732492</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.604461</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.264031</td>\n",
              "      <td>-0.960613</td>\n",
              "      <td>-0.142774</td>\n",
              "      <td>-0.488722</td>\n",
              "      <td>1.294578</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.926957</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.314893</td>\n",
              "      <td>-0.960613</td>\n",
              "      <td>-1.154031</td>\n",
              "      <td>-1.392789</td>\n",
              "      <td>1.820074</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.604461</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.009721</td>\n",
              "      <td>0.038063</td>\n",
              "      <td>0.699941</td>\n",
              "      <td>0.415345</td>\n",
              "      <td>0.769081</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.926957</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.213169</td>\n",
              "      <td>-0.960613</td>\n",
              "      <td>-0.142774</td>\n",
              "      <td>-0.488722</td>\n",
              "      <td>1.469743</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.604461</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.213169</td>\n",
              "      <td>-0.960613</td>\n",
              "      <td>-0.142774</td>\n",
              "      <td>-0.488722</td>\n",
              "      <td>1.469743</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.604461</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.857134</td>\n",
              "      <td>0.370955</td>\n",
              "      <td>0.194312</td>\n",
              "      <td>0.415345</td>\n",
              "      <td>1.031829</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.926957</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.857134</td>\n",
              "      <td>0.370955</td>\n",
              "      <td>0.194312</td>\n",
              "      <td>0.415345</td>\n",
              "      <td>1.031829</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.926957</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.314893</td>\n",
              "      <td>-0.960613</td>\n",
              "      <td>-1.154031</td>\n",
              "      <td>-1.392789</td>\n",
              "      <td>1.820074</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.604461</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-07-31</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.009721</td>\n",
              "      <td>-0.183865</td>\n",
              "      <td>0.194312</td>\n",
              "      <td>-0.036688</td>\n",
              "      <td>1.294578</td>\n",
              "      <td>1.724334</td>\n",
              "      <td>0.926957</td>\n",
              "      <td>1.13112</td>\n",
              "      <td>1.113717</td>\n",
              "      <td>2.04105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Store  DayOfWeek  Year  Month  Day  StateHoliday  \\\n",
              "Date                                                           \n",
              "2015-07-31      1          5     3      7   31             1   \n",
              "2015-07-31      2          5     3      7   31             1   \n",
              "2015-07-31      3          5     3      7   31             1   \n",
              "2015-07-31      4          5     3      7   31             1   \n",
              "2015-07-31      5          5     3      7   31             1   \n",
              "2015-07-31      6          5     3      7   31             1   \n",
              "2015-07-31      7          5     3      7   31             1   \n",
              "2015-07-31      8          5     3      7   31             1   \n",
              "2015-07-31      9          5     3      7   31             1   \n",
              "2015-07-31     10          5     3      7   31             1   \n",
              "\n",
              "            CompetitionMonthsOpen  Promo2Weeks  StoreType  Assortment  \\\n",
              "Date                                                                    \n",
              "2015-07-31                     25            1          3           1   \n",
              "2015-07-31                     25           26          1           1   \n",
              "2015-07-31                     25           26          1           1   \n",
              "2015-07-31                     25            1          3           3   \n",
              "2015-07-31                      4            1          1           1   \n",
              "2015-07-31                     20            1          1           1   \n",
              "2015-07-31                     25            1          1           3   \n",
              "2015-07-31                     10            1          1           1   \n",
              "2015-07-31                     25            1          1           3   \n",
              "2015-07-31                     25            1          1           1   \n",
              "\n",
              "                ...        Min_Humidity  Max_Wind_SpeedKm_h  \\\n",
              "Date            ...                                           \n",
              "2015-07-31      ...           -1.620066            0.149027   \n",
              "2015-07-31      ...           -1.264031           -0.960613   \n",
              "2015-07-31      ...           -1.314893           -0.960613   \n",
              "2015-07-31      ...           -1.009721            0.038063   \n",
              "2015-07-31      ...           -1.213169           -0.960613   \n",
              "2015-07-31      ...           -1.213169           -0.960613   \n",
              "2015-07-31      ...           -0.857134            0.370955   \n",
              "2015-07-31      ...           -0.857134            0.370955   \n",
              "2015-07-31      ...           -1.314893           -0.960613   \n",
              "2015-07-31      ...           -1.009721           -0.183865   \n",
              "\n",
              "            Mean_Wind_SpeedKm_h  CloudCover     trend  trend_DE  \\\n",
              "Date                                                              \n",
              "2015-07-31            -0.142774   -1.844823  1.732492  1.724334   \n",
              "2015-07-31            -0.142774   -0.488722  1.294578  1.724334   \n",
              "2015-07-31            -1.154031   -1.392789  1.820074  1.724334   \n",
              "2015-07-31             0.699941    0.415345  0.769081  1.724334   \n",
              "2015-07-31            -0.142774   -0.488722  1.469743  1.724334   \n",
              "2015-07-31            -0.142774   -0.488722  1.469743  1.724334   \n",
              "2015-07-31             0.194312    0.415345  1.031829  1.724334   \n",
              "2015-07-31             0.194312    0.415345  1.031829  1.724334   \n",
              "2015-07-31            -1.154031   -1.392789  1.820074  1.724334   \n",
              "2015-07-31             0.194312   -0.036688  1.294578  1.724334   \n",
              "\n",
              "            AfterStateHoliday  BeforeStateHoliday     Promo  SchoolHoliday  \n",
              "Date                                                                        \n",
              "2015-07-31           0.604461             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.926957             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.604461             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.926957             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.604461             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.604461             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.926957             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.926957             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.604461             1.13112  1.113717        2.04105  \n",
              "2015-07-31           0.926957             1.13112  1.113717        2.04105  \n",
              "\n",
              "[10 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJAL0SFlufL2",
        "colab_type": "text"
      },
      "source": [
        "In time series data, cross-validation is not random. Instead, our holdout data is generally the most recent data, as it would be in real application. This issue is discussed in detail in this post on our web site.\n",
        "\n",
        "One approach is to take the last 25% of rows (sorted by date) as our validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic6-shEMuM9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ratio = 0.75\n",
        "# train_ratio = 0.9\n",
        "train_size = int(samp_size * train_ratio); train_size\n",
        "val_idx = list(range(train_size, len(df)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SVbukHjuj9c",
        "colab_type": "text"
      },
      "source": [
        "An even better option for picking a validation set is using the exact same length of time period as the test set uses - this is implemented here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUM5tKx8uhVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_idx = np.flatnonzero(\n",
        "    (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-qvS9gum7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_idx=[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkwq1EaZuslj",
        "colab_type": "text"
      },
      "source": [
        "For any Kaggle competitions, it is important that you have a strong understanding of your metric — how you are going to be judged. In this competition, we are going to be judged on Root Mean Square Percentage Error (RMSPE).\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*a7mJ5VCeuAxagGrHOq6ekQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL2DR5Mzuo2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inv_y(a): return np.exp(a)\n",
        "\n",
        "def exp_rmspe(y_pred, targ):\n",
        "    targ = inv_y(targ)\n",
        "    pct_var = (targ - inv_y(y_pred))/targ\n",
        "    return math.sqrt((pct_var**2).mean())\n",
        "\n",
        "max_log_y = np.max(yl)\n",
        "y_range = (0, max_log_y*1.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFU1XvABvBSC",
        "colab_type": "text"
      },
      "source": [
        "When you take the log of the data, getting the root mean squared error will actually get you the root mean square percentage error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUGBJlG6uyPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128,\n",
        "                                       test_df=df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elHBDXCTvLkG",
        "colab_type": "text"
      },
      "source": [
        "- As per usual, we will start by creating model data object which has a validation set, training set, and optional test set built into it. From that, we will get a learner, we will then optionally call lr_find, then call learn.fit and so forth.\n",
        "- The difference here is we are not using ImageClassifierData.from_csv or .from_paths, we need a different kind of model data called ColumnarModelData and we call from_data_frame.\n",
        "- PATH : Specifies where to store model files etc\n",
        "- val_idx : A list of the indexes of the rows that we want to put in the validation set\n",
        "- df : data frame that contains independent variable\n",
        "- yl : We took the dependent variable y returned by proc_df and took the log of that (i.e. np.log(y))\n",
        "- cat_flds : which columns to be treated as categorical. Remember, by this time, everything is a number, so unless we specify, it will treat them all as continuous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8c2v7e3xrCs",
        "colab_type": "text"
      },
      "source": [
        "For our continuous variables:\n",
        "\n",
        "The NN takes the vector of continuous variables, put it through mat mul, and decide how many cols we want and it spits out a new length 100 rank1 tensor. Put it through relu and another mat mul (mat product).\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*T604NRtHHBkBWFvWoovlUw.png)\n",
        "\n",
        "We may not even use softmax for regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAN4uRmuyLHr",
        "colab_type": "text"
      },
      "source": [
        "## What do we do about categorical variables?\n",
        "Categorical variables [50:49]\n",
        "**We create a new matrix of 7 rows and as many columns** as we choose (4, for example) and** fill it with floating numbers**. **To add “Sunday” to our rank 1 tensor with continuous variables, we do a look up to this matrix, which will return 4 floating numbers, and we use them as “Sunday”.** THIS is an embedding matrix.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*cAgCy5HfD0rvPDg2dQITeg.png)\n",
        "\n",
        "**Initially, these numbers are random. But we can put them through a neural net and update them in a way that reduces the loss.** Do gradient descent and update the embedding matrix to improve the weights of the matrix so it is less random and find the best weights for each day of the week. In other words, this **matrix is just another bunch of weights** in our neural net. And matrices of this type are called “embedding matrices”. **An embedding matrix is something where we start out with an integer between zero and maximum number of levels of that category.** We index into the matrix to find a particular row, and we append it to all of our continuous variables, and everything after that is just the same as before (linear → ReLU → etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhpAFhr_vDlg",
        "colab_type": "code",
        "outputId": "f548b0af-565d-4495-dcab-1903a75f4f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "cat_sz = [(c, len(joined_samp[c].cat.categories)+1) for c in cat_vars]\n",
        "#made a list of every categorical variable and its cardinality\n",
        "#cardinality - the number of elements in a set or other grouping, as a property of that grouping.\n",
        "cat_sz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Store', 1116),\n",
              " ('DayOfWeek', 8),\n",
              " ('Year', 4),\n",
              " ('Month', 13),\n",
              " ('Day', 32),\n",
              " ('StateHoliday', 3),\n",
              " ('CompetitionMonthsOpen', 26),\n",
              " ('Promo2Weeks', 27),\n",
              " ('StoreType', 5),\n",
              " ('Assortment', 4),\n",
              " ('PromoInterval', 4),\n",
              " ('CompetitionOpenSinceYear', 24),\n",
              " ('Promo2SinceYear', 9),\n",
              " ('State', 13),\n",
              " ('Week', 53),\n",
              " ('Events', 22),\n",
              " ('Promo_fw', 7),\n",
              " ('Promo_bw', 7),\n",
              " ('StateHoliday_fw', 4),\n",
              " ('StateHoliday_bw', 4),\n",
              " ('SchoolHoliday_fw', 9),\n",
              " ('SchoolHoliday_bw', 9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBAawDt0J99",
        "colab_type": "text"
      },
      "source": [
        "- Here is a list of every categorical variable and its cardinality.\n",
        "- Even if there were no missing values in the original data, you should still set aside one for unknown just in case.\n",
        "- The rule of thumb for determining the embedding size is the cardinality size divided by 2, but no bigger than 50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hIfUc9mvtG3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We use the cardinality of each variable (that is, its number of unique values) to decide how large to make its embeddings. Each level will be associated with a vector with length defined as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyjZt9JsvomT",
        "colab_type": "code",
        "outputId": "6eac647b-b2fb-481f-ddd0-adbe4218adc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
        "#take the cardinality of the variable divide it by 2 but dont make it bigger than 50\n",
        "#and that is the heuristic behind how many columns in the embedding matrix\n",
        "#By having higher dimensionality vector rather than just a single number, \n",
        "#it gives the deep learning network a chance to learn these rich representations.\n",
        "emb_szs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1116, 50),\n",
              " (8, 4),\n",
              " (4, 2),\n",
              " (13, 7),\n",
              " (32, 16),\n",
              " (3, 2),\n",
              " (26, 13),\n",
              " (27, 14),\n",
              " (5, 3),\n",
              " (4, 2),\n",
              " (4, 2),\n",
              " (24, 12),\n",
              " (9, 5),\n",
              " (13, 7),\n",
              " (53, 27),\n",
              " (22, 11),\n",
              " (7, 4),\n",
              " (7, 4),\n",
              " (4, 2),\n",
              " (4, 2),\n",
              " (9, 5),\n",
              " (9, 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEoROY_1wHX",
        "colab_type": "text"
      },
      "source": [
        "**Looking up an embedding with an index is identical to doing a matrix product between a one-hot encoded vector and the embedding matrix.** But doing so is terribly inefficient, so modern libraries implement this as taking an **integer** and **doing a look up into an array**.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*psxpwtr5bw55lKxVV_y81w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7qGnoUov272",
        "colab_type": "text"
      },
      "source": [
        "- Here, we are asking it to create a learner that is suitable for our model data.\n",
        "- 0.04 : how much dropout to use\n",
        "- [1000,500] : how many activations to have in each layer\n",
        "- [0.001,0.01] : how many dropout to use at later layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hRL3n2Y2Oep",
        "colab_type": "text"
      },
      "source": [
        "So for example, day of week now becomes eight rows by four columns embedding matrix. Conceptually this allows our model to create some interesting time series models. If there is something that has a seven day period cycle that goes up on Mondays and down on Wednesdays but only for daily and only in Berlin, it can totally do that — it has all the information it needs. This is a fantastic way to deal with time series. You just need to make sure that the cycle indicator in your time series exists as a column. If you did not have a column called day of week, it would be very difficult for the neural network to learn to do mod seven and look up in an embedding matrix. It is not impossible but really hard. If you are predicting sales of beverages in San Francisco, you probably want a list of when the ball game is on at AT&T park because that is going to to impact how many people are drinking beer in SoMa. So you need to make sure that the basic indicators or periodicity is in your data, and as long as they are there, neural net is going to learn to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4w3Xuwt0R1B",
        "colab_type": "text"
      },
      "source": [
        "Then pass the embedding size to the learner:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQkzkJpA2UAA",
        "colab_type": "text"
      },
      "source": [
        "- emb_szs : embedding size\n",
        "- len(df.columns)-len(cat_vars) : number of continuous variables in the data frame\n",
        "- 0.04 : embedding matrix has its own dropout and this is the dropout rate\n",
        "- 1 : how many outputs we want to create (output of the last linear layer)\n",
        "- [1000, 500] : number of activations in the first linear layer, and the second linear layer\n",
        "- [0.001, 0.01] : dropout in the first linear layer, and the second linear layer\n",
        "- y_range : we will not worry about that for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWXYrBQvvj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
        "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "m.summary()#doesnt work"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXHIVy4wN0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n",
        "                   [1000,500], [0.001,0.01], y_range=y_range)\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKRr79eH2e7S",
        "colab_type": "code",
        "outputId": "725d47b3-061c-4a4f-8547-490b823516a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe])\n",
        "#metrics : this is a custom metric which specifies a function to be called \n",
        "#at the end of every epoch and prints out a result\n",
        "#0.097 rmse !!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "996986a720a749479b7aec7f57cfff65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   exp_rmspe  \n",
            "    0      0.01328    0.000371   0.019455  \n",
            "    1      0.011348   0.00363    0.0621    \n",
            "    2      0.01002    0.008728   0.097925  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.00873]), 0.09792491196688566]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qzl-vD12kHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.fit(lr, 1, metrics=[exp_rmspe], cycle_len=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXCJnyqW3vR8",
        "colab_type": "text"
      },
      "source": [
        "There is a difference in a way we are calling get_learner. In imaging we just did Learner.trained and pass the data:\n",
        "\n",
        "learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True)\n",
        "\n",
        "For these kinds of models, in fact for a lot of the models, the model we build depends on the data. In this case, we need to know what embedding matrices we have. So in this case, the data objects creates the learner (upside down to what we have seen before):\n",
        "\n",
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n",
        "                   [1000,500], [0.001,0.01], y_range=y_range)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7VdzGO_3xKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Summary of steps (if you want to use this for your own dataset) [01:17:56]:\n",
        "\n",
        "Step 1. List categorical variable names, and list continuous variable names, and put them in a Pandas data frame\n",
        "\n",
        "Step 2. Create a list of which row indexes you want in your validation set\n",
        "\n",
        "Step 3. Call this exact line of code:\n",
        "\n",
        "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, \n",
        "         yl.astype(np.float32), cat_flds=cat_vars, bs=128, \n",
        "         test_df=df_test)\n",
        "Step 4. Create a list of how big you want each embedding matrix to be\n",
        "\n",
        "Step 5. Call get_learner — you can use these exact parameters to start with:\n",
        "\n",
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n",
        "                   [1000,500], [0.001,0.01], y_range=y_range)\n",
        "Step 6. Call m.fit\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjsncF2V34p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OPTIONAL\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "((val,trn), (y_val,y_trn)) = split_by_idx(val_idx, df.values, yl)\n",
        "\n",
        "m = RandomForestRegressor(n_estimators=40, max_features=0.99, min_samples_leaf=2,\n",
        "                          n_jobs=-1, oob_score=True)\n",
        "m.fit(trn, y_trn);\n",
        "preds = m.predict(val)\n",
        "m.score(trn, y_trn), m.score(val, y_val), m.oob_score_, exp_rmspe(preds, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_xgvC14zfe",
        "colab_type": "text"
      },
      "source": [
        "# NATURAL LANGUAGE PROCESSING\n",
        "\n",
        "One of the things you find in NLP is there are particular problems you can solve and they have particular names. There is a particular kind of problem in NLP called “language modeling” and it has a very specific definition — it means build a model where given a few words of a sentence, can you predict what the next word is going to be.\n",
        "\n",
        "## Language Modelling\n",
        "\n",
        "Here we have 18 months worth of papers from arXiv (arXiv.org) and this is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfBEeW_84iyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur6p34mZ5aTY",
        "colab_type": "text"
      },
      "source": [
        "' '.join(md.trn_ds[0].text[:150])\n",
        "\n",
        "'<cat> csni <summ> the exploitation of mm - wave bands is one of the key - enabler for 5 g mobile \\n radio networks . however , the introduction of mm - wave technologies in cellular \\n networks is not straightforward due to harsh propagation conditions that limit \\n the mm - wave access availability . mm - wave technologies require high - gain antenna \\n systems to compensate for high path loss and limited power . as a consequence , \\n directional transmissions must be used for cell discovery and synchronization \\n processes : this can lead to a non - negligible access delay caused by the \\n exploration of the cell area with multiple transmissions along different \\n directions . \\n    the integration of mm - wave technologies and conventional wireless access \\n networks with the objective of speeding up the cell search process requires new \\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J14tZaW5h-i",
        "colab_type": "text"
      },
      "source": [
        "<cat> — category of the paper. CSNI is Computer Science and Networking\n",
        "<summ> — abstract of the paper\n",
        "Here are what the output of a trained language model looks like. We did simple little tests in which you pass some priming text and see what the model thinks should come next:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGPy55a5nOm",
        "colab_type": "text"
      },
      "source": [
        "sample_model(m, \"<CAT> csni <SUMM> algorithms that\")\n",
        "...use the same network as a single node are not able to achieve the same performance as the traditional network - based routing algorithms . in this paper , we propose a novel routing scheme for routing protocols in wireless networks . the proposed scheme is based ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYA9kgrS52iC",
        "colab_type": "text"
      },
      "source": [
        "It learned by reading arXiv papers that somebody who is writing about computer networking would talk like this. Remember, it started out not knowing English at all. It started out with an embedding matrix for every word in English that was random. By reading lots of arXiv papers, it learned what kind of words followed others.\n",
        "\n",
        "Here we tried specifying a category to be computer vision:\n",
        "\n",
        "sample_model(m, \"<CAT> cscv <SUMM> algorithms that\")\n",
        "...use the same data to perform image classification are increasingly being used to improve the performance of image classification algorithms . in this paper , we propose a novel method for image classification using a deep convolutional neural network ( cnn ) . the proposed method is ...\n",
        "It not only learned how to write English pretty well, but also after you say something like “convolutional neural network” you should then use parenthesis to specify an acronym “(CNN)”.\n",
        "\n",
        "sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> on \")\n",
        "...the performance of deep learning for image classification <eos>\n",
        "sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> on \")\n",
        "...the performance of wireless networks <eos>\n",
        "sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> towards \")\n",
        "...a new approach to image classification <eos>\n",
        "sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> towards \")\n",
        "...a new approach to the analysis of wireless networks <eos>\n",
        "A language model can be incredibly deep and subtle, so we are going to try and build that — not because we care about this at all, but because we are trying to create a pre-trained model which is used to do some other tasks. For example, given an IMDB movie review, we will figure out whether they are positive or negative. It is a lot like cats vs. dogs — a classification problem. So we would really like to use a pre-trained network which at least knows how to read English. So we will train a model that predicts a next word of a sentence (i.e. language model), and just like in computer vision, stick some new layers on the end and ask it to predict whether something is positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EmZm-vq-i66",
        "colab_type": "text"
      },
      "source": [
        "# IMDB\n",
        "\n",
        "What we are going to do is to train a language model, making that the pre-trained model for a classification model. In other words, we are trying to leverage exactly what we learned in our computer vision which is how to do fine-tuning to create powerful classification models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn3Ohai65Nnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from fastai.learner import *\n",
        "\n",
        "import torchtext\n",
        "from torchtext import vocab, data\n",
        "from torchtext.datasets import language_modeling\n",
        "\n",
        "from fastai.rnn_reg import *\n",
        "from fastai.rnn_train import *\n",
        "from fastai.nlp import *\n",
        "from fastai.lm_rnn import *\n",
        "\n",
        "import dill as pickle\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Tj3Qyk-nJl",
        "colab_type": "code",
        "outputId": "19871134-b583-4be1-86f1-174d69753611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://files.fast.ai/data/aclImdb.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-14 09:49:12--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [text/plain]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  41.1MB/s    in 3.9s    \n",
            "\n",
            "2018-10-14 09:49:16 (36.1 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJVhImSo-sHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xzvf aclImdb.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykco0RYN_O8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mv aclImdb data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9rSqR3w-wlP",
        "colab_type": "code",
        "outputId": "243ecb82-6277-4c40-af97-4a50635cd34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "PATH='data/aclImdb/'\n",
        "\n",
        "TRN_PATH = 'train/all/'\n",
        "VAL_PATH = 'test/all/'\n",
        "TRN = f'{PATH}{TRN_PATH}'\n",
        "VAL = f'{PATH}{VAL_PATH}'\n",
        "\n",
        "%ls {PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqxIhHNZAfIf",
        "colab_type": "code",
        "outputId": "efdb80d5-4f01-4e16-b917-68c77b37567e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "trn_files = !ls {TRN}\n",
        "#Let's look inside the training folder...\n",
        "\n",
        "trn_files[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0_0.txt       1562_10.txt  24997_0.txt\\t34371_0.txt  43748_0.txt  6248_7.txt',\n",
              " '0_3.txt       15621_0.txt  24998_0.txt\\t3437_1.txt   43749_0.txt  6249_0.txt',\n",
              " '0_9.txt       1562_1.txt   24999_0.txt\\t34372_0.txt  437_4.txt\\t  6249_2.txt',\n",
              " '10000_0.txt   15622_0.txt  25000_0.txt\\t34373_0.txt  43750_0.txt  6249_7.txt',\n",
              " '10000_4.txt   15623_0.txt  2500_0.txt\\t34374_0.txt  4375_0.txt   624_9.txt',\n",
              " '10000_8.txt   15624_0.txt  25001_0.txt\\t34375_0.txt  43751_0.txt  6250_0.txt',\n",
              " '1000_0.txt    15625_0.txt  2500_1.txt\\t34376_0.txt  4375_1.txt   6250_10.txt',\n",
              " '10001_0.txt   15626_0.txt  25002_0.txt\\t34377_0.txt  43752_0.txt  6250_1.txt',\n",
              " '10001_10.txt  15627_0.txt  25003_0.txt\\t34378_0.txt  43753_0.txt  625_0.txt',\n",
              " '10001_4.txt   15628_0.txt  25004_0.txt\\t3437_8.txt   43754_0.txt  625_10.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBXNMfxyAf4U",
        "colab_type": "code",
        "outputId": "a1a15e14-e161-4809-8dd6-645145c789ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "review = !cat {TRN}{trn_files[6]}#...and at an example review.\n",
        "\n",
        "\n",
        "review[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop-socky fung-ku, but what I got instead was a comedy. So, it wasn't quite was I was expecting, but I really liked it anyway! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them!! I was laughing my ass off. I mean, the cops were just so bad! And when I say bad, I mean The Shield Vic Macky bad. But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose...man, oh man. What can you say about that hottie. She was great and put those other actresses to shame. She should work more often!!!!! I also really liked the fight scene outside of the building. That was done really well. Lots of fighting and people getting their heads banged up. FUN! Last, but not least Joe Estevez and William Smith were great as the...well, I wasn't sure what they were, but they seemed to be having fun and throwing out lines. I mean, some of it didn't make sense with the rest of the flick, but who cares when you're laughing so hard! All in all the film wasn't the greatest thing since sliced bread, but I wasn't expecting that. It was a Troma flick so I figured it would totally suck. It's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies. Don't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat: 15625_0.txt: No such file or directory\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4JthrpoBMaG",
        "colab_type": "code",
        "outputId": "79236f73-bfe4-4e26-e583-99b79a133bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Now we'll check how many words are in the dataset.\n",
        "\n",
        "!find {TRN} -name '*.txt' | xargs cat | wc -w\n",
        "\n",
        "!find {VAL} -name '*.txt' | xargs cat | wc -w\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17486581\n",
            "5686719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ekQoKEbEZE7",
        "colab_type": "text"
      },
      "source": [
        "Before we can do anything with text, we have to turn it into a list of tokens. Token is basically like a word. Eventually we will turn them into a list of numbers, but the first step is to turn it into a list of words — this is called “tokenization” in NLP. A good tokenizer will do a good job of recognizing pieces in your sentence. Each separated piece of punctuation will be separated, and each part of multi-part word will be separated as appropriate. Spacy does a lot of NLP stuff, and it has the best tokenizer Jeremy knows. So Fast.ai library is designed to work well with the Spacey tokenizer as with torchtext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQq6_NuREhVS",
        "colab_type": "code",
        "outputId": "39a30138-aeca-4f1e-c8dd-70a52d2ba55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "! python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 48.5MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b03u6OW5D0b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_tok = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RQE_XVZEf8w",
        "colab_type": "code",
        "outputId": "335c797a-9852-40e2-886b-a475bca4d152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\n",
        "' '.join([sent.string.strip() for sent in spacy_tok(review[0])])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop - socky fung - ku , but what I got instead was a comedy . So , it was n't quite was I was expecting , but I really liked it anyway ! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them ! ! I was laughing my ass off . I mean , the cops were just so bad ! And when I say bad , I mean The Shield Vic Macky bad . But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose ... man , oh man . What can you say about that hottie . She was great and put those other actresses to shame . She should work more often ! ! ! ! ! I also really liked the fight scene outside of the building . That was done really well . Lots of fighting and people getting their heads banged up . FUN ! Last , but not least Joe Estevez and William Smith were great as the ... well , I was n't sure what they were , but they seemed to be having fun and throwing out lines . I mean , some of it did n't make sense with the rest of the flick , but who cares when you 're laughing so hard ! All in all the film was n't the greatest thing since sliced bread , but I was n't expecting that . It was a Troma flick so I figured it would totally suck . It 's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies . Do n't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat : 15625_0.txt : No such file or directory\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdGUd4FFFQKa",
        "colab_type": "text"
      },
      "source": [
        "Creating a field [01:41:01]\n",
        "A field is a definition of how to pre-process some text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDAw5v6HEp_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(lower=True, tokenize=\"spacy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7L1cVp2Fn8Q",
        "colab_type": "text"
      },
      "source": [
        "- lower=True — lowercase the text\n",
        "- tokenize=spacy_tok — tokenize with spacy_tok\n",
        "\n",
        "Now we create the usual Fast.ai model data object:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzfTaHuiFllj",
        "colab_type": "text"
      },
      "source": [
        "fastai works closely with torchtext. We create a ModelData object for language modeling by taking advantage of LanguageModelData, passing it our torchtext field object, and the paths to our training, test, and validation sets. In this case, we don't have a separate test set, so we'll just use VAL_PATH for that too.\n",
        "\n",
        "As well as the usual bs (batch size) parameter, we also now have bptt; this define how many words are processing at a time in each row of the mini-batch. More importantly, it defines how many 'layers' we will backprop through. Making this number higher will increase time and memory requirements, but will improve the model's ability to handle long sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKxNR98wFU76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=64; bptt=70\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5wH1PMnFzem",
        "colab_type": "text"
      },
      "source": [
        "- PATH : as per usual where the data is, where to save models, etc\n",
        "- TEXT : torchtext’s Field definition\n",
        "- **FILES : list of all of the files we have: training, validation, and test (to keep things simple, we do not have a separate validation and test set, so both points to validation folder)\n",
        "- bs : batch size\n",
        "- bptt : Back Prop Through Time. It means how long a sentence we will stick on the GPU at once\n",
        "- min_freq=10 : In a moment, we are going to be replacing words with integers (a unique index for every word). If there are any words that occur less than 10 times, just call it unknown.\n",
        "\n",
        "\n",
        "After building our ModelData object, it automatically fills the TEXT object with a very important attribute: TEXT.vocab. This is a vocabulary, which stores which unique words (or tokens) have been seen in the text, and how each word will be mapped to a unique integer id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ3X4BqMFszx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
        "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW8r4NMoFuiz",
        "colab_type": "code",
        "outputId": "1a4cbafb-b1ff-486b-f863-3726d766b7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))\n",
        "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)\n",
        "#Here are the: # batches; # unique tokens in the vocab; length of the dataset; # of words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4583, 37392, 1, 20540756)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8GtDqI3HDOO",
        "colab_type": "text"
      },
      "source": [
        "itos is sorted by frequency except for the first two special ones. Using vocab, torchtext will turn words into integer IDs for us :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gVtS1o7GtbX",
        "colab_type": "code",
        "outputId": "a5f68e39-1569-458c-ba2c-04cbb8a1e2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 'itos': 'int-to-string'\n",
        "TEXT.vocab.itos[:12]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzGNTDYTGvyo",
        "colab_type": "code",
        "outputId": "b82974e0-054d-4b6b-e21a-3c878c5d953f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 'stoi': 'string to int'\n",
        "TEXT.vocab.stoi['the']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9KCPcF8G7uu",
        "colab_type": "code",
        "outputId": "3e6d5014-3359-4ca2-8ae7-726d458c653f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "md.trn_ds[0].text[:12]\n",
        "#Note that in a LanguageModelData object there is only one item \n",
        "#in each dataset: all the words of the text joined together."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['depressing',\n",
              " 'and',\n",
              " 'meaningless',\n",
              " 'pap',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'like',\n",
              " 'one',\n",
              " 'of',\n",
              " 'those',\n",
              " 'french']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_getVEWHHBI",
        "colab_type": "code",
        "outputId": "d2290d31-0406-4a5b-9777-a9f74af15eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#torchtext will handle turning this words into integer IDs for us automatically.\n",
        "TEXT.numericalize([md.trn_ds[0].text[:12]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable containing:\n",
              "  2086\n",
              "     5\n",
              "  3913\n",
              " 16845\n",
              "     4\n",
              "    11\n",
              "     9\n",
              "    47\n",
              "    37\n",
              "     7\n",
              "   164\n",
              "   723\n",
              "[torch.cuda.LongTensor of size 12x1 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23lfgqFHUsa",
        "colab_type": "text"
      },
      "source": [
        "Our LanguageModelData object will create batches with 64 columns (that's our batch size), and varying sequence lengths of around 80 tokens (that's our bptt parameter - backprop through time).\n",
        "\n",
        "Each batch also contains the exact same data as labels, but one word later in the text - since we're trying to always predict the next word. The labels are flattened into a 1d array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm894LhJHeea",
        "colab_type": "text"
      },
      "source": [
        "## Batch size and BPTT [01:47:40]\n",
        "What happens in a language model is even though we have lots of movie reviews, they all get concatenated together into one big block of text. So we predict the next word in this huge long thing which is all of the IMDB movie reviews concatenated together.\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*O-Kq1qtgZmrShbKhaN3fTg.png)\n",
        "\n",
        "- We split up the concatenated reviews into batches. In this case, we will split it to 64 sections\n",
        "- We then move each section underneath the previous one, and transpose it.\n",
        "- We end up with a matrix which is 1 million by 64.\n",
        "- We then grab a little chunk at time and those chunk lengths are approximately equal to BPTT. Here, we grab a little 70 long section and that is the first thing we chuck into our GPU (i.e. the batch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV-1RSonIicM",
        "colab_type": "text"
      },
      "source": [
        "- We grab our first training batch by wrapping data loader with iter then calling next.\n",
        "- We got back a 75 by 64 tensor (approximately 70 rows but not exactly)\n",
        "- A neat trick torchtext does is to randomly change the bptt number every time so each epoch it is getting slightly different bits of text — similar to shuffling images in computer vision. We cannot randomly shuffle the words because they need to be in the right order, so instead, we randomly move their breakpoints a little bit.\n",
        "- The target value is also 75 by 64 but for minor technical reasons it is flattened out into a single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Turhw1tNHP-k",
        "colab_type": "code",
        "outputId": "556dd95e-bd2c-40ca-d251-12c202490fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "next(iter(md.trn_dl))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Variable containing:\n",
              "   2086    148     44  ...     321      3      3\n",
              "      5    502  10927  ...       7    113      5\n",
              "   3913    158   2863  ...       2     11      2\n",
              "         ...            ⋱           ...         \n",
              "     65     27  33329  ...     186      3   2370\n",
              "    860      6     17  ...     596  10687    905\n",
              "      4    314     41  ...      20      3    931\n",
              " [torch.cuda.LongTensor of size 72x64 (GPU 0)], Variable containing:\n",
              "      5\n",
              "    502\n",
              "  10927\n",
              "   ⋮   \n",
              "      2\n",
              "      5\n",
              "  14962\n",
              " [torch.cuda.LongTensor of size 4608 (GPU 0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWH2mzQkI8hl",
        "colab_type": "text"
      },
      "source": [
        "# Create a model\n",
        "\n",
        "\n",
        "This is what our embedding matrix looks like:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*6EHxqeSYMioiLEQ5ufrf_g.png)\n",
        "\n",
        "- It is a high cardinality categorical variable and furthermore, it is the only variable — this is typical in NLP\n",
        "- The embedding size is 200 which is much bigger than our previous embedding vectors. Not surprising because a word has a lot more nuance to it than the concept of Sunday. Generally, an embedding size for a word will be somewhere between 50 and 600."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umnral6NHagI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "em_sz = 200  # size of each embedding vector\n",
        "nh = 500     # number of hidden activations per layer\n",
        "nl = 3       # number of layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSjhmT_JJSep",
        "colab_type": "text"
      },
      "source": [
        "Researchers have found that large amounts of momentum (which we’ll learn about later) don’t work well with these kinds of RNN models, so we create a version of the Adam optimizer with less momentum than its default of 0.9. Any time you are doing NLP, you should probably include this line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pYhntcmJOiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHVj5V-_Jhb4",
        "colab_type": "text"
      },
      "source": [
        "Fast.ai uses a variant of the state of the art AWD LSTM Language Model developed by Stephen Merity. A key feature of this model is that it provides excellent regularization through Dropout. There is no simple way known (yet!) to find the best values of the dropout parameters below — you just have to experiment…\n",
        "\n",
        "However, the other parameters (alpha, beta, and clip) shouldn't generally need tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYZtJK48Jc-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner = md.get_model(opt_fn, em_sz, nh, nl, dropouti=0.05,\n",
        "                       dropout=0.05, wdrop=0.1, dropoute=0.02, \n",
        "                       dropouth=0.05)\n",
        "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "learner.clip=0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohPu1I1lKAbi",
        "colab_type": "text"
      },
      "source": [
        "- There is another kind of way we can avoid overfitting that we will talk about in the last class. For now, learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) works reliably so all of your NLP models probably want this particular line.\n",
        "- learner.clip=0.3 : when you look at your gradients and you multiply them by the learning rate to decide how much to update your weights by, this will not allow them be more than 0.3. This is a cool little trick to prevent us from taking too big of a step.\n",
        "- Details do not matter too much right now, so you can use them as they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-LoebIaJ0g5",
        "colab_type": "code",
        "outputId": "705205db-3ab3-4d7b-e684-556bd672cc35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cddcb76dcd6e4a3d911a67adde2483fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 1833/4583 [08:29<13:04,  3.50it/s, loss=5.19]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-357a8890c905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOm9v-oKOUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save_encoder('adam3_20_enc')\n",
        "learner.load_encoder('adam3_20_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ljb7aMKU5k",
        "colab_type": "text"
      },
      "source": [
        "Language modeling accuracy is generally measured using the metric perplexity, which is simply exp() of the loss function we used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gakhYA2iKXuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "math.exp(4.165)\n",
        "#pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4MCqyCKUh9",
        "colab_type": "text"
      },
      "source": [
        "## Testing [02:04:53]\n",
        "We can play around with our language model a bit to check it seems to be working OK. First, let’s create a short bit of text to ‘prime’ a set of predictions. We’ll use our torchtext field to numericalize it so we can feed it to our language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jICfRIpYKhb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=learner.model\n",
        "ss=\"\"\". So, it wasn't quite was I was expecting, but I really liked it anyway! The best\"\"\"\n",
        "s = [TEXT.preprocess(ss)]\n",
        "t=TEXT.numericalize(s)\n",
        "' '.join(s[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXCmhikeLFdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We haven't yet added methods to make it easy to test a language model, \n",
        "#so we'll need to manually go through the steps.\n",
        "# Set batch size to 1\n",
        "m[0].bs=1\n",
        "# Turn off dropout\n",
        "m.eval()\n",
        "# Reset hidden state\n",
        "m.reset()\n",
        "# Get predictions from model\n",
        "res,*_ = m(t)\n",
        "# Put the batch size back to what it was\n",
        "m[0].bs=bs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffumnwIiLMdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's see what the top 10 predictions were for the next word after our short text:\n",
        "\n",
        "nexts = torch.topk(res[-1], 10)[1]\n",
        "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv-MqWsTLQVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#...and let's see if our model can generate a bit more text all by itself!\n",
        "\n",
        "print(ss,\"\\n\")\n",
        "for i in range(50):\n",
        "    n=res[-1].topk(2)[1]\n",
        "    n = n[1] if n.data[0]==0 else n[0]\n",
        "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
        "    res,*_ = m(n[0].unsqueeze(0))\n",
        "print('...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Q0aqdYLW05",
        "colab_type": "text"
      },
      "source": [
        "Sentiment [02:05:09]\n",
        "So we had pre-trained a language model and now we want to fine-tune it to do sentiment classification.\n",
        "\n",
        "To use a pre-trained model, we will need to the saved vocab from the language model, since we need to ensure the same words map to the same IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaciHskOLZzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ7n62ceLdBE",
        "colab_type": "text"
      },
      "source": [
        "sequential=False tells torchtext that a text field should be tokenized (in this case, we just want to store the 'positive' or 'negative' single label).\n",
        "\n",
        "This time, we need to not treat the whole thing as one big piece of text but every review is separate because each one has a different sentiment attached to it.\n",
        "\n",
        "splits is a torchtext method that creates train, test, and validation sets. The IMDB dataset is built into torchtext, so we can take advantage of that. Take a look at lang_model-arxiv.ipynb to see how to define your own fastai/torchtext datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qml4OpbDLifS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "IMDB_LABEL = data.Field(sequential=False)\n",
        "splits = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')\n",
        "t = splits[0].examples[0]\n",
        "t.label, ' '.join(t.text[:16])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAP4f0X9Ln5K",
        "colab_type": "text"
      },
      "source": [
        "fastai can create a ModelData object directly from torchtext splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_k8LQ9zLoXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "md2 = TextData.from_splits(PATH, splits, bs)\n",
        "#Now you can go ahead and call get_model that gets us our learner. \n",
        "#Then we can load into it the pre-trained language model (load_encoder).\n",
        "m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, \n",
        "           dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3)\n",
        "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "m3.load_encoder(f'adam3_10_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9EVUkImLwea",
        "colab_type": "text"
      },
      "source": [
        "Because we’re fine-tuning a pretrained model, we’ll use differential learning rates, and also increase the max gradient for clipping, to allow the SGDR to work better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKy9P4qALw6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m3.clip=25.\n",
        "lrs=np.array([1e-4,1e-3,1e-2])\n",
        "m3.freeze_to(-1)\n",
        "m3.fit(lrs/2, 1, metrics=[accuracy])\n",
        "m3.unfreeze()\n",
        "m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMe4hHTL3DT",
        "colab_type": "text"
      },
      "source": [
        "We make sure all except the last layer is frozen. Then we train a bit, unfreeze it, train it a bit. The nice thing is once you have got a pre-trained language model, it actually trains really fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwmd9YDkL3bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')\n",
        "m3.load_cycle('imdb2', 4)\n",
        "accuracy_np(*m3.predict_with_targs())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGIe7QYgL5zx",
        "colab_type": "text"
      },
      "source": [
        "A recent paper from Bradbury et al, Learned in translation: contextualized word vectors, has a handy summary of the latest academic research in solving this IMDB sentiment analysis problem. Many of the latest algorithms shown are tuned for this specific problem.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*PotEPJjvS-R4C5OCMbw7Vw.png)\n"
      ]
    }
  ]
}